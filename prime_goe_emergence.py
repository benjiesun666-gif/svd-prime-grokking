"""
ç´ æ•°é—´éš™é¢„æµ‹ - çº¯ç²¹æ¶Œç°ç‰ˆ
Pure Emergence Approach to Prime Number Patterns

æ ¸å¿ƒç†å¿µï¼š
- ä¸é¢„è®¾ä»»ä½•å®ˆæ’å®šå¾‹
- ä¸äººä¸ºå¼•å¯¼èƒ½é‡æˆ–å­¦ä¹ ç‡
- è®©AIåœ¨"ç”Ÿå­˜å‹åŠ›"ä¸‹è‡ªç„¶è¿›åŒ–
- åœ¨é¡¿æ‚Ÿæ—¶åˆ»ä¿å­˜æƒé‡
- äº‹åè§£æç¥ç»ç½‘ç»œå¯»æ‰¾æ·±å±‚æ•°å­¦ç»“æ„

ç›®æ ‡ï¼šé€šè¿‡AIè‡ªå·±çš„"æ™ºæ…§"å‘ç°ç´ æ•°çš„æ·±å±‚è§„å¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.amp import autocast, GradScaler
import numpy as np
import math
from sympy import primerange
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy import stats, interpolate
import time
import json
import os
from pathlib import Path

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR = Path("riemann_pure_emergence_results")
OUTPUT_DIR.mkdir(exist_ok=True)

print("=" * 70)
print("ğŸŒŸ ç´ æ•°é—´éš™é¢„æµ‹AIå®éªŒ - çº¯ç²¹æ¶Œç°ç‰ˆ")
print("=" * 70)
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    gpu_props = torch.cuda.get_device_properties(0)
    print(f"æ˜¾å­˜: {gpu_props.total_memory / 1e9:.2f} GB")
else:
    print("âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰")
    # Kaggleç¯å¢ƒè‡ªåŠ¨ç»§ç»­ï¼ˆä¸éœ€è¦äº¤äº’ï¼‰
    import sys
    if 'ipykernel' not in sys.modules:  # éJupyterç¯å¢ƒæ‰è¯¢é—®
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            exit()

print("=" * 70)

# ==================== é…ç½® ====================
class Config:
    """å®éªŒé…ç½® - çº¯ç²¹ç‰ˆï¼ˆæ— å®ˆæ’çº¦æŸï¼‰"""
    
    # æ•°æ®é…ç½®
    NUM_PRIMES = 1000000  # ğŸ”¥ 100ä¸‡ç´ æ•°
    
    # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
    USE_GRADIENT_ACCUMULATION = True
    PHYSICAL_BATCH_SIZE = 128       # ç‰©ç†batchï¼ˆæ˜¾å­˜å®é™…å ç”¨ï¼‰
    ACCUMULATION_STEPS = 16         # ç´¯ç§¯æ­¥æ•°
    # ç­‰æ•ˆbatch = 128 Ã— 16 = 2048ï¼ˆä¿æŒè®­ç»ƒæ•ˆæœï¼‰
    
    # è§£é‡Šï¼š
    # ä¼ ç»Ÿæ–¹æ³•ï¼šbatch=2048 â†’ æ˜¾å­˜çˆ†ç‚¸
    # æ¢¯åº¦ç´¯ç§¯ï¼šç‰©ç†batch=128ï¼Œç´¯ç§¯16æ¬¡ â†’ æ•ˆæœç­‰ä»·ï¼Œæ˜¾å­˜å®‰å…¨
    # 
    # ç±»æ¯”ï¼š
    # ä¼ ç»Ÿ = ä¸€æ¬¡åƒ16ä¸ªåŒ…å­ï¼ˆæ’‘æ­»ï¼‰
    # ç´¯ç§¯ = åˆ†16æ¬¡åƒï¼Œæ¯æ¬¡1ä¸ªï¼ˆèˆ’æœï¼‰
    
    # æ¨¡å‹é…ç½®ï¼ˆå¯è¢«å…ƒè¿›åŒ–ä¼˜åŒ–ï¼‰
    D_MODEL = 256        # å‡å°æ¨¡å‹ä»¥åŠ é€Ÿï¼ˆä»æœ‰è¶³å¤Ÿè¡¨è¾¾åŠ›ï¼‰
    N_LAYERS = 6
    N_HEADS = 8
    DROPOUT = 0.1
    LEARNABLE_EMBEDDING = True  # ğŸ”¥ ç»ˆææŒ‘æˆ˜ï¼šè®©AIä»éšæœºå™ªå£°ä¸­æ¶Œç°æ•°å­¦è§„å¾‹ï¼
    
    # è®­ç»ƒé…ç½®ï¼ˆå¯è¢«å…ƒè¿›åŒ–ä¼˜åŒ–ï¼‰
    NUM_EPOCHS = 10000     # ğŸ”¥ 10000è½®ï¼ˆ100ä¸‡æ•°æ®å…¨é‡è®­ç»ƒï¼‰
    BATCH_SIZE = 256       # ğŸ”¥ Kaggle GPUæ˜¾å­˜æ›´å¤§ï¼Œå¯ä»¥ç”¨256
    LEARNING_RATE = 1e-4   # åŸºå‡†å­¦ä¹ ç‡
    WEIGHT_DECAY = 0.01
    
    # å­¦ä¹ ç‡ç­–ç•¥ï¼ˆå…ƒè¿›åŒ–å¯é€‰æ‹©ï¼‰
    LR_SCHEDULE = 'cosine'  # 'constant', 'cosine', 'plateau'
    
    # ç›‘æ§é…ç½®
    PRINT_EVERY = 1  # æ¯è½®éƒ½æ‰“å°
    SAVE_EVERY = 500
    
    # é¡¿æ‚Ÿæ£€æµ‹ï¼ˆçº¯è§‚å¯Ÿï¼Œä¸å¹²é¢„ï¼‰
    GROKKING_THRESHOLD = 0.3  # å•è½®Lossä¸‹é™>30%
    GROKKING_WINDOW = 20      # æ£€æµ‹çª—å£
    
    # æ¶Œç°è¿½è¸ª
    TRACK_GRADIENTS = True    # è®°å½•æ¢¯åº¦ä¿¡æ¯
    TRACK_WEIGHTS = True      # è®°å½•æƒé‡ç»Ÿè®¡
    
    # ğŸ”¥ æ–­ç‚¹ç»­å­˜é…ç½®
    CHECKPOINT_EVERY = 50      # æ¯50ä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint
    CHECKPOINT_DIR = "riemann_checkpoints"  # checkpointä¿å­˜ç›®å½•
    AUTO_RESUME = True         # è‡ªåŠ¨ä»æœ€æ–°checkpointæ¢å¤

config = Config()

# ==================== æ–­ç‚¹ç»­å­˜ç³»ç»Ÿ ====================
def save_checkpoint(epoch, model, optimizer, losses, tracker, hyperparam_evolver, config, filename):
    """ä¿å­˜è®­ç»ƒcheckpoint"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'losses': losses,
        'tracker_history': tracker.history if tracker else None,
        'hyperparam_history': hyperparam_evolver.evolution_history if hyperparam_evolver else None,
        'config': {
            'NUM_PRIMES': config.NUM_PRIMES,
            'D_MODEL': config.D_MODEL,
            'N_LAYERS': config.N_LAYERS,
            'N_HEADS': config.N_HEADS,
            'LEARNING_RATE': config.LEARNING_RATE,
            'BATCH_SIZE': config.BATCH_SIZE,
            'ACCUMULATION_STEPS': config.ACCUMULATION_STEPS,
        }
    }
    torch.save(checkpoint, filename)
    print(f"ğŸ’¾ Checkpointå·²ä¿å­˜: {filename} (Epoch {epoch})")

def load_checkpoint(filename, model, optimizer, device):
    """åŠ è½½è®­ç»ƒcheckpoint"""
    if not os.path.exists(filename):
        return None
    
    print(f"ğŸ“‚ åŠ è½½checkpoint: {filename}")
    checkpoint = torch.load(filename, map_location=device, weights_only=False)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    print(f"âœ… æˆåŠŸæ¢å¤åˆ° Epoch {checkpoint['epoch']}")
    print(f"   ç´¯è®¡Losså†å²: {len(checkpoint['losses'])}æ¡")
    
    return checkpoint

def find_latest_checkpoint(checkpoint_dir):
    """æŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶"""
    if not os.path.exists(checkpoint_dir):
        return None
    
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_') and f.endswith('.pt')]
    if not checkpoints:
        return None
    
    # æå–epochç¼–å·å¹¶æ’åº
    epochs = [int(f.split('_')[2].split('.')[0]) for f in checkpoints]
    latest_epoch = max(epochs)
    latest_file = f'checkpoint_epoch_{latest_epoch}.pt'
    
    return os.path.join(checkpoint_dir, latest_file)

# ==================== åœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨ ====================
class OnlineHyperparamEvolution:
    """
    åœ¨çº¿è¶…å‚æ•°è¿›åŒ– - è¾¹è®­ç»ƒè¾¹è¿›åŒ–
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ä¸é¢„å…ˆå›ºå®šè¶…å‚æ•°
    2. æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´
    3. è¶…å‚æ•°æœ¬èº«ä¹Ÿæ˜¯"æ¶Œç°"çš„äº§ç‰©
    """
    def __init__(self, initial_lr=1e-4, initial_wd=0.01):
        self.hyperparams = {
            'lr': initial_lr,
            'weight_decay': initial_wd,
        }
        
        # åŠ¨æ€å†·å´æœŸï¼ˆåŸºäºè®­ç»ƒç¨³å®šæ€§ï¼‰
        self.base_update_interval = 50  # åŸºå‡†é—´éš”
        self.current_cooldown = 30      # åˆæœŸè¾ƒçŸ­ï¼Œå¿«é€Ÿæ¢ç´¢
        self.cooldown_bounds = (20, 150)  # å†·å´æœŸèŒƒå›´
        
        self.last_update_epoch = -100
        
        # å­¦ä¹ ç‡èŒƒå›´
        self.lr_bounds = (1e-6, 5e-4)
        self.max_change_ratio = 0.3  # å•æ¬¡æœ€å¤§å˜åŒ–30%
        
        # å†å²è®°å½•
        self.evolution_history = []
        
        print(f"\n{'='*70}")
        print("ğŸ§¬ åœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨å·²å¯åŠ¨")
        print(f"{'='*70}")
        print(f"åˆå§‹å­¦ä¹ ç‡: {initial_lr:.2e}")
        print(f"åˆå§‹æƒé‡è¡°å‡: {initial_wd}")
        print(f"åŠ¨æ€å†·å´æœŸ: {self.current_cooldown} (èŒƒå›´: {self.cooldown_bounds})")
        print(f"{'='*70}\n")
    
    def calculate_dynamic_cooldown(self, losses):
        """
        åŠ¨æ€è®¡ç®—å†·å´æœŸ
        
        ç­–ç•¥ï¼šåŸºäºLossçš„ç¨³å®šæ€§
        - Losså¾ˆç¨³å®š â†’ cooldowné•¿ï¼ˆä¸éœ€è¦é¢‘ç¹è°ƒæ•´ï¼‰
        - Lossæ³¢åŠ¨å¤§ â†’ cooldownçŸ­ï¼ˆéœ€è¦å¿«é€Ÿå“åº”ï¼‰
        """
        if len(losses) < 50:
            return 30  # åˆæœŸé»˜è®¤
        
        recent_losses = losses[-50:]
        loss_mean = np.mean(recent_losses)
        loss_std = np.std(recent_losses)
        
        # å˜å¼‚ç³»æ•°ï¼ˆCVï¼‰
        cv = loss_std / (loss_mean + 1e-10)
        
        # æ ¹æ®CVè°ƒæ•´cooldown
        if cv < 0.03:  # éå¸¸ç¨³å®š
            new_cooldown = 100
        elif cv < 0.08:  # ä¸­ç­‰ç¨³å®š
            new_cooldown = 50
        elif cv < 0.15:  # è½»å¾®æ³¢åŠ¨
            new_cooldown = 30
        else:  # é«˜æ³¢åŠ¨
            new_cooldown = 20
        
        # é™åˆ¶èŒƒå›´
        new_cooldown = np.clip(new_cooldown, *self.cooldown_bounds)
        
        # å¹³æ»‘å˜åŒ–ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
        alpha = 0.3
        smoothed_cooldown = int(alpha * new_cooldown + (1 - alpha) * self.current_cooldown)
        
        return smoothed_cooldown
    
    def should_update(self, epoch, losses):
        """æ˜¯å¦åº”è¯¥æ›´æ–°è¶…å‚æ•°"""
        # æ›´æ–°åŠ¨æ€å†·å´æœŸ
        self.current_cooldown = self.calculate_dynamic_cooldown(losses)
        
        # åˆ¤æ–­æ˜¯å¦åˆ°äº†æ›´æ–°æ—¶æœº
        epochs_since_last = epoch - self.last_update_epoch
        return epochs_since_last >= self.current_cooldown and epoch >= 30
    
    def evaluate_progress(self, losses):
        """è¯„ä¼°æœ€è¿‘çš„è®­ç»ƒè¿›åº¦"""
        if len(losses) < 50:
            return 0.0
        
        recent_50 = losses[-50:]
        previous_50 = losses[-100:-50] if len(losses) >= 100 else losses[:50]
        
        recent_avg = np.mean(recent_50)
        previous_avg = np.mean(previous_50)
        
        # è¿›åº¦ = ç›¸å¯¹ä¸‹é™å¹…åº¦
        if previous_avg < 1e-10:
            return 0.0
        
        progress = (previous_avg - recent_avg) / previous_avg
        return progress
    
    def mutate_hyperparams(self, progress, losses):
        """
        æ ¹æ®è¿›åº¦å˜å¼‚è¶…å‚æ•°
        
        ç­–ç•¥ï¼š
        - è¿›åº¦å¥½ â†’ å°å¹…å˜å¼‚ï¼ˆä¿æŒæ–¹å‘ï¼‰
        - è¿›åº¦å·® â†’ å¤§å¹…å˜å¼‚ï¼ˆå¯»æ‰¾æ–°æ–¹å‘ï¼‰
        """
        if progress > 0.05:  # è‰¯å¥½è¿›å±•ï¼ˆLossä¸‹é™>5%ï¼‰
            mutation_strength = 0.1
            strategy = "ä¿æŒæ–¹å‘"
        elif progress > 0.01:  # å¾®å¼±è¿›å±•
            mutation_strength = 0.2
            strategy = "é€‚åº¦æ¢ç´¢"
        else:  # åœæ»æˆ–å€’é€€
            mutation_strength = 0.4
            strategy = "å¤§èƒ†çªç ´"
        
        # å˜å¼‚å­¦ä¹ ç‡
        lr_multiplier = np.random.uniform(
            1 - mutation_strength,
            1 + mutation_strength
        )
        new_lr = self.hyperparams['lr'] * lr_multiplier
        
        # é™åˆ¶ç»å¯¹èŒƒå›´
        new_lr = np.clip(new_lr, *self.lr_bounds)
        
        # é™åˆ¶å•æ¬¡å˜åŒ–å¹…åº¦
        max_change = self.hyperparams['lr'] * self.max_change_ratio
        new_lr = np.clip(
            new_lr,
            self.hyperparams['lr'] - max_change,
            self.hyperparams['lr'] + max_change
        )
        
        return {
            'lr': new_lr,
            'weight_decay': self.hyperparams['weight_decay'],
            'strategy': strategy,
            'mutation_strength': mutation_strength
        }
    
    def update(self, epoch, losses, optimizer):
        """æ›´æ–°è¶…å‚æ•°"""
        if not self.should_update(epoch, losses):
            return False
        
        # è¯„ä¼°è¿›åº¦
        progress = self.evaluate_progress(losses)
        
        # å˜å¼‚è¶…å‚æ•°
        new_hyperparams = self.mutate_hyperparams(progress, losses)
        
        # è®°å½•å†å²
        self.evolution_history.append({
            'epoch': epoch,
            'old_lr': self.hyperparams['lr'],
            'new_lr': new_hyperparams['lr'],
            'progress': progress,
            'strategy': new_hyperparams['strategy'],
            'cooldown': self.current_cooldown
        })
        
        # æ›´æ–°ä¼˜åŒ–å™¨ï¼ˆä¸é‡å»ºï¼Œä¿ç•™AdamçŠ¶æ€ï¼‰
        old_lr = self.hyperparams['lr']
        for param_group in optimizer.param_groups:
            param_group['lr'] = new_hyperparams['lr']
            param_group['weight_decay'] = new_hyperparams['weight_decay']
        
        # æ‰“å°è¿›åŒ–ä¿¡æ¯
        lr_change = (new_hyperparams['lr'] - old_lr) / old_lr * 100
        
        print(f"\n{'ğŸ§¬'*35}")
        print(f"ğŸ’¥ è¶…å‚æ•°è¿›åŒ–è§¦å‘ - Epoch {epoch}")
        print(f"{'='*70}")
        print(f"  è®­ç»ƒè¿›åº¦: {progress*100:+.2f}% (æœ€è¿‘50è½® vs å‰50è½®)")
        print(f"  è¿›åŒ–ç­–ç•¥: {new_hyperparams['strategy']}")
        print(f"  å˜å¼‚å¼ºåº¦: {new_hyperparams['mutation_strength']*100:.0f}%")
        print(f"  å­¦ä¹ ç‡: {old_lr:.2e} â†’ {new_hyperparams['lr']:.2e} ({lr_change:+.1f}%)")
        print(f"  åŠ¨æ€å†·å´æœŸ: {self.current_cooldown} è½®")
        print(f"  ä¸‹æ¬¡è¿›åŒ–: çº¦ Epoch {epoch + self.current_cooldown}")
        print(f"{'='*70}")
        print(f"{'ğŸ§¬'*35}\n")
        
        self.hyperparams = {
            'lr': new_hyperparams['lr'],
            'weight_decay': new_hyperparams['weight_decay']
        }
        self.last_update_epoch = epoch
        
        return True

# ==================== æ¶Œç°è¿½è¸ªå™¨ ====================
class EmergenceTracker:
    """
    æ¶Œç°è¿½è¸ªå™¨ - çº¯è§‚å¯Ÿï¼Œä¸å¹²é¢„
    
    èŒè´£ï¼š
    1. è®°å½•è®­ç»ƒå…¨è¿‡ç¨‹çš„å…³é”®æŒ‡æ ‡
    2. æ£€æµ‹é¡¿æ‚Ÿæ—¶åˆ»ï¼ˆç”¨äºä¿å­˜æƒé‡ï¼‰
    3. äº‹ååˆ†ææ¶Œç°çš„è§„å¾‹
    """
    def __init__(self):
        self.phi = (1 + np.sqrt(5)) / 2
        
        self.history = {
            'epoch': [],
            'loss': [],
            'lr': [],
            'grad_norm': [],
            'param_norm': [],
        }
        
        self.grokking_moments = []  # è®°å½•æ‰€æœ‰é¡¿æ‚Ÿæ—¶åˆ»
    
    def record(self, epoch, loss, lr, grad_norm=None, param_norm=None):
        """è®°å½•å½“å‰epochçš„æŒ‡æ ‡"""
        self.history['epoch'].append(epoch)
        self.history['loss'].append(loss)
        self.history['lr'].append(lr)
        if grad_norm is not None:
            self.history['grad_norm'].append(grad_norm)
        if param_norm is not None:
            self.history['param_norm'].append(param_norm)
    
    def detect_grokking(self, config, model=None, X_sample=None, y_sample=None):
        """
        æ£€æµ‹é¡¿æ‚Ÿæ—¶åˆ»ï¼ˆå¤šç»´åº¦æ•°å­¦çªç ´æ£€æµ‹ï¼‰
        
        ä¸åªæ£€æµ‹Lossçªé™ï¼Œæ›´æ£€æµ‹ï¼š
        1. Lossçªå˜ï¼ˆä¼ ç»Ÿé¡¿æ‚Ÿï¼‰
        2. å¯¹æ•°å…³ç³»å‘ç°ï¼ˆlog correlationï¼‰
        3. å‘¨æœŸæ€§æ¨¡å¼ï¼ˆFFT dominant frequencyï¼‰
        
        å¤šä¸ªç»´åº¦åŒæ—¶æ»¡è¶³æ‰æ˜¯"çœŸé¡¿æ‚Ÿ"ï¼
        """
        losses = self.history['loss']
        
        if len(losses) < config.GROKKING_WINDOW + 1:
            return False, 0.0
        
        # ==================== ç»´åº¦1ï¼šLossçªå˜æ£€æµ‹ ====================
        recent_avg = np.mean(losses[-(config.GROKKING_WINDOW+1):-1])
        current_loss = losses[-1]
        
        sudden_drop = recent_avg - current_loss
        drop_ratio = sudden_drop / recent_avg if recent_avg > 1e-10 else 0
        
        loss_breakthrough = (drop_ratio > config.GROKKING_THRESHOLD and 
                            sudden_drop > 0.01 and 
                            current_loss < recent_avg)
        
        # ==================== ç»´åº¦2ï¼šå¯¹æ•°å…³ç³»æ£€æµ‹ ====================
        log_breakthrough = False
        log_corr = 0.0
        
        if model is not None and X_sample is not None and y_sample is not None:
            try:
                with torch.no_grad():
                    predictions = model(X_sample).squeeze()
                    
                    # è®¡ç®—å¯¹æ•°ç©ºé—´çš„ç›¸å…³æ€§
                    # å¦‚æœAIå‘ç°äº†å¯¹æ•°è§„å¾‹ï¼Œpredå’Œtrueåœ¨logç©ºé—´åº”è¯¥é«˜åº¦ç›¸å…³
                    pred_np = predictions.cpu().numpy()
                    true_np = y_sample.cpu().numpy().squeeze()
                    
                    # é¿å…log(0)
                    pred_log = np.log(np.abs(pred_np) + 1e-6)
                    true_log = np.log(np.abs(true_np) + 1e-6)
                    
                    # è®¡ç®—ç›¸å…³ç³»æ•°
                    if len(pred_log) > 10 and np.std(pred_log) > 1e-6 and np.std(true_log) > 1e-6:
                        log_corr = np.corrcoef(pred_log, true_log)[0, 1]
                        
                        # å¯¹æ•°ç›¸å…³æ€§>0.9è®¤ä¸ºå‘ç°äº†å¯¹æ•°è§„å¾‹
                        log_breakthrough = (log_corr > 0.9)
            
            except Exception as e:
                # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ£€æµ‹
                pass
        
        # ==================== ç»´åº¦3ï¼šå‘¨æœŸæ€§æ£€æµ‹ ====================
        periodicity_breakthrough = False
        dominant_freq = 0.0
        
        if model is not None and X_sample is not None:
            try:
                with torch.no_grad():
                    predictions = model(X_sample).squeeze()
                    pred_np = predictions.cpu().numpy()
                    
                    # å»é™¤è¶‹åŠ¿ï¼ˆä½¿ç”¨å·®åˆ†ï¼‰
                    if len(pred_np) > 100:
                        pred_detrended = np.diff(pred_np)
                        
                        # FFTåˆ†æ
                        fft_result = np.fft.fft(pred_detrended)
                        freqs = np.fft.fftfreq(len(pred_detrended))
                        
                        # åªçœ‹æ­£é¢‘ç‡
                        positive_freqs = freqs[:len(freqs)//2]
                        positive_fft = np.abs(fft_result[:len(freqs)//2])
                        
                        if len(positive_fft) > 1:
                            # æ‰¾åˆ°ä¸»é¢‘ç‡
                            main_idx = np.argmax(positive_fft[1:]) + 1  # è·³è¿‡DCåˆ†é‡
                            dominant_freq = positive_freqs[main_idx]
                            
                            # å¦‚æœä¸»é¢‘ç‡çš„å¹…åº¦è¿œå¤§äºå¹³å‡ï¼ˆ>5å€ï¼‰ï¼Œè®¤ä¸ºå‘ç°äº†å‘¨æœŸæ€§
                            mean_amplitude = np.mean(positive_fft[1:])
                            max_amplitude = positive_fft[main_idx]
                            
                            if max_amplitude > 5 * mean_amplitude:
                                periodicity_breakthrough = True
            
            except Exception as e:
                pass
        
        # ==================== ç»¼åˆåˆ¤æ–­ ====================
        # è‡³å°‘æ»¡è¶³ä¸¤ä¸ªç»´åº¦æ‰ç®—"çœŸé¡¿æ‚Ÿ"
        breakthrough_count = sum([loss_breakthrough, log_breakthrough, periodicity_breakthrough])
        
        is_true_grokking = breakthrough_count >= 2
        
        # å¦‚æœæ˜¯çœŸé¡¿æ‚Ÿï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
        if is_true_grokking:
            print(f"\n{'='*70}")
            print(f"ğŸ”¬ å¤šç»´åº¦æ•°å­¦çªç ´æ£€æµ‹")
            print(f"{'='*70}")
            print(f"  âœ“ Lossçªå˜: {'æ˜¯' if loss_breakthrough else 'å¦'} (ä¸‹é™{drop_ratio*100:.1f}%)")
            if log_corr != 0.0:
                print(f"  âœ“ å¯¹æ•°å…³ç³»: {'æ˜¯' if log_breakthrough else 'å¦'} (ç›¸å…³æ€§={log_corr:.4f})")
            if dominant_freq != 0.0:
                print(f"  âœ“ å‘¨æœŸæ€§: {'æ˜¯' if periodicity_breakthrough else 'å¦'} (ä¸»é¢‘ç‡={dominant_freq:.6f})")
            print(f"  â†’ çªç ´ç»´åº¦: {breakthrough_count}/3")
            print(f"{'='*70}\n")
        
        return is_true_grokking, drop_ratio
    
    def analyze_emergence(self):
        """
        äº‹ååˆ†æï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ¶Œç°äº†ä»€ä¹ˆè§„å¾‹ï¼Ÿ
        
        çº¯ç²¹è®°å½• - ä¸é¢„è®¾ä»»ä½•å®ˆæ’å®šå¾‹
        """
        print("\n" + "="*70)
        print("ğŸ” æ¶Œç°åˆ†æ")
        print("="*70)
        
        epochs = np.array(self.history['epoch'])
        losses = np.array(self.history['loss'])
        lrs = np.array(self.history['lr'])
        
        # åªè®°å½•åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“‰ è®­ç»ƒç»Ÿè®¡ï¼š")
        print(f"  åˆå§‹Loss: {losses[0]:.6f}")
        print(f"  æœ€ç»ˆLoss: {losses[-1]:.6f}")
        print(f"  ä¸‹é™å¹…åº¦: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%")
        print(f"  é¡¿æ‚Ÿæ¬¡æ•°: {len(self.grokking_moments)}")
        
        if self.grokking_moments:
            print(f"  é¡¿æ‚Ÿæ—¶åˆ»: {self.grokking_moments}")
        
        # å­¦ä¹ ç‡ç»Ÿè®¡
        print(f"\nğŸ“Š å­¦ä¹ ç‡æ¼”åŒ–ï¼š")
        print(f"  åˆå§‹LR: {lrs[0]:.2e}")
        print(f"  æœ€ç»ˆLR: {lrs[-1]:.2e}")
        print(f"  LRèŒƒå›´: [{lrs.min():.2e}, {lrs.max():.2e}]")
        
        # æ¢¯åº¦ç»Ÿè®¡
        if self.history['grad_norm']:
            grad_norms = np.array(self.history['grad_norm'])
            print(f"\nğŸ“ˆ æ¢¯åº¦ç»Ÿè®¡ï¼š")
            print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(grad_norms):.4f}")
            print(f"  æœ€å¤§æ¢¯åº¦èŒƒæ•°: {np.max(grad_norms):.4f}")
            print(f"  æœ€å°æ¢¯åº¦èŒƒæ•°: {np.min(grad_norms):.4f}")
        
        print("\n" + "="*70)
        
        return {
            'loss_reduction': (losses[0] - losses[-1]) / losses[0],
            'grokking_count': len(self.grokking_moments),
            'final_loss': losses[-1],
            'final_lr': lrs[-1]
        }

# ==================== æ•°æ®ç”Ÿæˆ ====================
def generate_prime_gaps(num_primes):
    """ç”Ÿæˆç´ æ•°é—´éš™æ•°æ®"""
    print(f"\n{'='*70}")
    print(f"ç”Ÿæˆå‰ {num_primes:,} ä¸ªç´ æ•°...")
    print(f"{'='*70}")
    start_time = time.time()
    
    # ä¼°ç®—ä¸Šç•Œ
    if num_primes < 10:
        upper_bound = 30
    else:
        ln_n = math.log(num_primes)
        ln_ln_n = math.log(ln_n) if ln_n > 1 else 0
        upper_bound = int(num_primes * (ln_n + ln_ln_n + 2))
    
    print(f"ä¼°ç®—ä¸Šç•Œ: {upper_bound:,}")
    
    # ç”Ÿæˆç´ æ•°
    primes = list(primerange(1, upper_bound))
    
    if len(primes) < num_primes:
        print(f"âš ï¸  è­¦å‘Šï¼šåªç”Ÿæˆäº† {len(primes)} ä¸ªç´ æ•°")
        num_primes = len(primes)
    else:
        primes = primes[:num_primes]
    
    # è®¡ç®—é—´éš™
    prime_gaps = np.diff(primes)
    
    elapsed = time.time() - start_time
    print(f"âœ“ ç”Ÿæˆå®Œæˆ ({elapsed:.2f}ç§’)")
    print(f"  ç´ æ•°æ•°é‡: {len(primes):,}")
    print(f"  é—´éš™æ•°é‡: {len(prime_gaps):,}")
    print(f"  é—´éš™èŒƒå›´: [{prime_gaps.min()}, {prime_gaps.max()}]")
    print(f"  å¹³å‡é—´éš™: {np.mean(prime_gaps):.2f}")
    print(f"  å‰20ä¸ªé—´éš™: {list(prime_gaps[:20])}")
    
    return prime_gaps

# ==================== æ¨¡å‹å®šä¹‰ ====================
class RiemannEmbedding(nn.Module):
    """
    ä½ç½®ç¼–ç æ¨¡å—
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. å›ºå®šæ­£å¼¦ç¼–ç ï¼ˆæ ‡å‡†Transformerï¼‰
    2. å¯å­¦ä¹ ç¼–ç ï¼ˆè®©AIè‡ªå·±æ¶Œç°é¢‘ç‡ï¼‰
    """
    def __init__(self, d_model, max_len=1000000, learnable=False):
        super().__init__()
        self.d_model = d_model
        self.learnable = learnable
        
        if learnable:
            # å¯å­¦ä¹ ç¼–ç ï¼šè®©AIè‡ªå·±å‘ç°log(p)è§„å¾‹
            self.embedding = nn.Embedding(max_len, d_model)
            # åˆå§‹åŒ–ä¸ºå°éšæœºå€¼
            nn.init.normal_(self.embedding.weight, mean=0, std=0.02)
        else:
            # å›ºå®šæ­£å¼¦ç¼–ç 
            pe = torch.zeros(max_len, d_model)
            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
            
            div_term = torch.exp(
                torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
            )
            
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            
            self.register_buffer('pe', pe)
    
    def forward(self, x):
        if self.learnable:
            return self.embedding(x)
        else:
            return self.pe[x]

class PrimeGapPredictor(nn.Module):
    """ç´ æ•°é—´éš™é¢„æµ‹å™¨"""
    def __init__(self, d_model=512, n_layers=6, n_heads=8, dropout=0.1, learnable_embedding=False):
        super().__init__()
        self.d_model = d_model
        
        self.riemann_embedding = RiemannEmbedding(d_model, learnable=learnable_embedding)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            norm_first=True,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        self.output = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, d_model // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 4, 1)
        )
    
    def forward(self, x):
        embedded = self.riemann_embedding(x)
        embedded = embedded.unsqueeze(1)
        transformed = self.transformer(embedded)
        gap = self.output(transformed.squeeze(1))
        return gap
    
    def get_hidden_states(self, x):
        """æå–éšè—å±‚çŠ¶æ€ï¼ˆç”¨äºGOEåˆ†æï¼‰"""
        embedded = self.riemann_embedding(x)
        embedded = embedded.unsqueeze(1)
        hidden = self.transformer(embedded)
        return hidden.squeeze(1)
    
    def get_attention_weights(self):
        """æå–æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºGOEåˆ†æï¼‰"""
        weights = []
        for layer in self.transformer.layers:
            # æå–self-attentionçš„æƒé‡
            attn_weights = layer.self_attn.in_proj_weight
            weights.append(attn_weights.detach().cpu())
        return torch.cat(weights, dim=0)

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_model(model, X_gpu_full, y_gpu_full, device, config, tracker, hyperparam_evolver=None):
    """
    çº¯ç²¹è®­ç»ƒ - ä¸åŠ ä»»ä½•äººä¸ºçº¦æŸ
    
    å”¯ä¸€ç›®æ ‡ï¼šæœ€å°åŒ–Loss
    è®©AIè‡ªå·±æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥
    
    å¦‚æœæä¾›hyperparam_evolverï¼Œåˆ™å¯ç”¨åœ¨çº¿è¶…å‚æ•°è¿›åŒ–
    
    ğŸ”¥ æ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰ï¼š
       100ä¸‡æ•°æ®å…¨é‡è®­ç»ƒï¼Œä½†ä½¿ç”¨å°batch + æ¢¯åº¦ç´¯ç§¯
       â†’ ç‰©ç†batch = 128ï¼ˆæ˜¾å­˜å‹å¥½ï¼‰
       â†’ é€»è¾‘batch = 128 Ã— 16 = 2048ï¼ˆæ•ˆæœç­‰ä»·ï¼‰
       â†’ è®©100ä¸‡æ•°æ®åœ¨8GBæ˜¾å­˜ä¸‹å®‰å…¨è¿è¡Œï¼
    """
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY
    )
    
    # å¦‚æœå¯ç”¨åœ¨çº¿è¿›åŒ–ï¼Œä¸ä½¿ç”¨ä¼ ç»Ÿè°ƒåº¦å™¨
    scheduler = None
    if hyperparam_evolver is None:
        if config.LR_SCHEDULE == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=config.NUM_EPOCHS,
                eta_min=1e-6
            )
        elif config.LR_SCHEDULE == 'plateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=0.5,
                patience=100,
                min_lr=1e-6
            )
    
    criterion = nn.MSELoss()
    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ï¼šä½¿ç”¨å…¨éƒ¨æ•°æ®
    X_gpu = X_gpu_full
    y_gpu = y_gpu_full
    total_data_size = len(X_gpu)
    
    # è®¡ç®—batchæ•°é‡ï¼ˆåŸºäºç‰©ç†batch sizeï¼‰
    num_samples = total_data_size
    num_batches = (num_samples + config.BATCH_SIZE - 1) // config.BATCH_SIZE
    
    # æ¢¯åº¦ç´¯ç§¯é…ç½®
    if config.USE_GRADIENT_ACCUMULATION:
        accumulation_steps = config.ACCUMULATION_STEPS
        effective_batch_size = config.BATCH_SIZE * accumulation_steps
        print(f"\n{'ğŸ’ª'*35}")
        print(f"ğŸ’ª æ¢¯åº¦ç´¯ç§¯è®­ç»ƒï¼ˆ100ä¸‡æ•°æ®å…¨é‡ï¼Œç¡¬æ°”ï¼ï¼‰")
        print(f"{'='*70}")
        print(f"  æ€»æ•°æ®é‡: {total_data_size:,}")
        print(f"  ç‰©ç†batch: {config.BATCH_SIZE}")
        print(f"  ç´¯ç§¯æ­¥æ•°: {accumulation_steps}")
        print(f"  ç­‰æ•ˆbatch: {effective_batch_size}")
        print(f"  ç­–ç•¥: å°æ­¥å¿«è·‘ï¼Œæ•ˆæœç­‰ä»·ï¼Œæ˜¾å­˜å®‰å…¨")
        print(f"  é¢„è®¡æ˜¾å­˜: ~3-4GBï¼ˆå®‰å…¨ï¼‰")
        print(f"{'='*70}")
        print(f"{'ğŸ’ª'*35}\n")
    else:
        accumulation_steps = 1
        effective_batch_size = config.BATCH_SIZE
    
    losses = []
    best_loss = float('inf')
    start_epoch = 0  # èµ·å§‹epochï¼ˆæ–­ç‚¹ç»­å­˜ä¼šä¿®æ”¹ï¼‰
    
    # ğŸ”¥ æ–­ç‚¹ç»­å­˜ï¼šå°è¯•æ¢å¤
    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)
    if config.AUTO_RESUME:
        latest_checkpoint = find_latest_checkpoint(config.CHECKPOINT_DIR)
        if latest_checkpoint:
            print(f"\n{'ğŸ’¾'*35}")
            print(f"ğŸ’¾ æ£€æµ‹åˆ°checkpointï¼Œæ­£åœ¨æ¢å¤...")
            print(f"{'='*70}")
            checkpoint_data = load_checkpoint(latest_checkpoint, model, optimizer, device)
            if checkpoint_data:
                start_epoch = checkpoint_data['epoch'] + 1
                losses = checkpoint_data['losses']
                if tracker and checkpoint_data.get('tracker_history'):
                    tracker.history = checkpoint_data['tracker_history']
                if hyperparam_evolver and checkpoint_data.get('hyperparam_history'):
                    hyperparam_evolver.evolution_history = checkpoint_data['hyperparam_history']
                print(f"âœ… è®­ç»ƒå°†ä» Epoch {start_epoch} ç»§ç»­")
                print(f"{'='*70}")
                print(f"{'ğŸ’¾'*35}\n")
            else:
                print("âš ï¸  checkpointåŠ è½½å¤±è´¥ï¼Œä»å¤´å¼€å§‹")
    
    print(f"\n{'='*70}")
    print("ğŸš€ å¼€å§‹è®­ç»ƒ - çº¯ç²¹æ¶Œç°æ¨¡å¼")
    print(f"{'='*70}")
    print(f"è®­ç»ƒæ ·æœ¬: {num_samples:,}")
    print(f"Batchå¤§å°: {config.BATCH_SIZE}")
    print(f"Batchæ•°é‡: {num_batches}")
    if hyperparam_evolver is not None:
        print(f"è¶…å‚æ•°ç­–ç•¥: ğŸ§¬ åœ¨çº¿è¿›åŒ–ï¼ˆåŠ¨æ€å†·å´ï¼‰")
    else:
        print(f"å­¦ä¹ ç‡ç­–ç•¥: {config.LR_SCHEDULE}")
    print(f"å¤šç»´åº¦æ•°å­¦çªç ´æ£€æµ‹: âœ… å·²å¯ç”¨")
    print(f"æ–­ç‚¹ç»­å­˜: æ¯{config.CHECKPOINT_EVERY}è½®ä¿å­˜ â†’ {config.CHECKPOINT_DIR}/")
    if start_epoch > 0:
        print(f"ç»­å­˜æ¨¡å¼: ä» Epoch {start_epoch} ç»§ç»­ï¼ˆå·²å®Œæˆ{start_epoch}è½®ï¼‰")
    print(f"{'='*70}")
    print(f"\nâ³ è®­ç»ƒå³å°†å¼€å§‹ï¼ˆæ¯è½®éƒ½ä¼šæ‰“å°è¿›åº¦ï¼‰...\n")
    
    start_time = time.time()
    epoch_times = []
    
    for epoch in range(start_epoch, config.NUM_EPOCHS):
        model.train()
        epoch_loss = 0.0
        epoch_start = time.time()
        
        # ç¬¬ä¸€è½®ç‰¹åˆ«æç¤º
        if epoch == 0:
            print(f"ğŸ”¥ Epoch 0 å¼€å§‹è®­ç»ƒ...")
            print(f"   æ€»æ•°æ®é‡: {total_data_size:,}")
            print(f"   ç‰©ç†batch: {config.BATCH_SIZE}")
            print(f"   ç´¯ç§¯æ­¥æ•°: {accumulation_steps}")
            print(f"   ç­‰æ•ˆbatch: {effective_batch_size}")
            print(f"   ï¼ˆå¦‚æœçœ‹åˆ°è¿™æ¡æ¶ˆæ¯åé•¿æ—¶é—´æ— ååº”ï¼Œè¯´æ˜batchè®¡ç®—å¾ˆæ…¢ï¼‰\n")
        
        # è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
        optimizer.zero_grad()  # ğŸ”¥ æ”¾åˆ°epochå¼€å§‹ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * config.BATCH_SIZE
            end_idx = min(start_idx + config.BATCH_SIZE, num_samples)
            
            batch_X = X_gpu[start_idx:end_idx]
            batch_y = y_gpu[start_idx:end_idx]
            
            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
            
            # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ï¼šç¼©æ”¾loss
            if config.USE_GRADIENT_ACCUMULATION:
                loss = loss / accumulation_steps
            
            scaler.scale(loss).backward()
            
            # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ï¼šæ¯Næ­¥æ›´æ–°ä¸€æ¬¡
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == num_batches:
                scaler.unscale_(optimizer)
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            
            epoch_loss += loss.item() * (accumulation_steps if config.USE_GRADIENT_ACCUMULATION else 1)
        
        # è®°å½•
        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)
        
        # åœ¨çº¿è¶…å‚æ•°è¿›åŒ–ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if hyperparam_evolver is not None:
            hyperparam_evolver.update(epoch, losses, optimizer)
        
        # ä¼ ç»Ÿå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä»…åœ¨æœªå¯ç”¨è¿›åŒ–æ—¶ï¼‰
        if scheduler is not None and hyperparam_evolver is None:
            if config.LR_SCHEDULE == 'plateau':
                scheduler.step(epoch_loss / num_batches)
            else:
                scheduler.step()
        
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®¡ç®—å‚æ•°èŒƒæ•°
        param_norm = sum(p.norm().item() ** 2 for p in model.parameters()) ** 0.5
        
        # è®°å½•åˆ°è¿½è¸ªå™¨
        tracker.record(epoch, avg_loss, current_lr, grad_norm.item(), param_norm)
        
        # æ£€æµ‹é¡¿æ‚Ÿï¼ˆå¤šç»´åº¦æ•°å­¦çªç ´æ£€æµ‹ï¼‰
        # å‡†å¤‡é‡‡æ ·æ•°æ®ç”¨äºæ•°å­¦åˆ†æ
        sample_size = min(1000, num_samples)
        sample_indices = torch.randperm(num_samples)[:sample_size].to(device)
        X_sample = X_gpu[sample_indices]
        y_sample = y_gpu[sample_indices]
        
        is_grokking, drop_ratio = tracker.detect_grokking(config, model, X_sample, y_sample)
        
        if is_grokking:
            tracker.grokking_moments.append(epoch)
            print(f"\n{'ğŸ”¥'*35}")
            print(f"ğŸ’¥ æ£€æµ‹åˆ°é¡¿æ‚Ÿï¼Epoch {epoch}")
            print(f"  Lossçªé™: {drop_ratio*100:.1f}%")
            print(f"  å½“å‰Loss: {avg_loss:.6f}")
            print(f"{'ğŸ”¥'*35}\n")
            
            # ğŸ”¬ ä¿å­˜é¡¿æ‚Ÿå‰çš„æƒé‡ï¼ˆç”¨äºå¯¹æ¯”ç›¸å˜ï¼‰
            if epoch >= 10:
                print(f"  ğŸ’¾ ä¿å­˜é¡¿æ‚Ÿå‰æƒé‡ï¼ˆEpoch {epoch-10}ï¼‰ç”¨äºç›¸å˜åˆ†æ...")
                # æ³¨æ„ï¼šè¿™é‡Œåªèƒ½ä¿å­˜å½“å‰æ¨¡å‹ï¼Œæ— æ³•å›æº¯å†å²
                # åœ¨å®é™…å®ç°ä¸­ï¼Œéœ€è¦åœ¨è®­ç»ƒæ—¶æŒç»­ä¿å­˜æœ€è¿‘çš„æƒé‡
            
            # ä¿å­˜é¡¿æ‚Ÿæ—¶åˆ»çš„æƒé‡
            save_grokking_weights(model, epoch, avg_loss, OUTPUT_DIR, label='after')
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'loss': avg_loss,
            }, OUTPUT_DIR / 'best_model.pt')
        
        # ğŸ”¥ æ–­ç‚¹ç»­å­˜ï¼šå®šæœŸä¿å­˜checkpoint
        if (epoch + 1) % config.CHECKPOINT_EVERY == 0:
            checkpoint_path = os.path.join(
                config.CHECKPOINT_DIR,
                f'checkpoint_epoch_{epoch}.pt'
            )
            save_checkpoint(
                epoch, model, optimizer, losses,
                tracker, hyperparam_evolver, config,
                checkpoint_path
            )
        
        # æ‰“å°è¿›åº¦
        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)
        
        if epoch % config.PRINT_EVERY == 0 or is_grokking:
            avg_epoch_time = np.mean(epoch_times[-50:]) if epoch_times else epoch_time
            remaining_epochs = config.NUM_EPOCHS - epoch - 1
            eta_seconds = remaining_epochs * avg_epoch_time
            eta_minutes = eta_seconds / 60
            
            speed = 1.0 / avg_epoch_time if avg_epoch_time > 0 else 0
            
            print(f"Epoch {epoch:5d}/{config.NUM_EPOCHS} | "
                  f"Loss={avg_loss:.6f} | "
                  f"LR={current_lr:.2e} | "
                  f"GradNorm={grad_norm:.2f} | "
                  f"é€Ÿåº¦={speed:.2f}ep/s | "
                  f"ETA={eta_minutes:.1f}min")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config.SAVE_EVERY == 0 and epoch > 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'losses': losses,
            }, OUTPUT_DIR / f'checkpoint_epoch_{epoch}.pt')
    
    total_time = time.time() - start_time
    print(f"\n{'='*70}")
    print(f"âœ“ è®­ç»ƒå®Œæˆï¼")
    print(f"  æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"  æœ€ä½³Loss: {best_loss:.6f}")
    print(f"  é¡¿æ‚Ÿæ¬¡æ•°: {len(tracker.grokking_moments)}")
    
    # åœ¨çº¿è¿›åŒ–ç»Ÿè®¡
    if hyperparam_evolver is not None:
        print(f"\nğŸ§¬ è¶…å‚æ•°è¿›åŒ–ç»Ÿè®¡:")
        print(f"  è¿›åŒ–æ¬¡æ•°: {len(hyperparam_evolver.evolution_history)}")
        if hyperparam_evolver.evolution_history:
            final_lr = hyperparam_evolver.hyperparams['lr']
            initial_lr = config.LEARNING_RATE
            print(f"  å­¦ä¹ ç‡: {initial_lr:.2e} â†’ {final_lr:.2e}")
            
            # ä¿å­˜è¿›åŒ–å†å²
            np.save(OUTPUT_DIR / 'hyperparam_evolution_history.npy', 
                   hyperparam_evolver.evolution_history)
    
    print(f"{'='*70}\n")
    
    return losses

def save_grokking_weights(model, epoch, loss, output_dir, label=''):
    """
    ä¿å­˜é¡¿æ‚Ÿæ—¶åˆ»çš„æƒé‡ï¼ˆç”¨äºGOEåˆ†æå’Œç›¸å˜å¯¹æ¯”ï¼‰
    
    label: 'before'ï¼ˆé¡¿æ‚Ÿå‰ï¼‰æˆ– 'after'ï¼ˆé¡¿æ‚Ÿåï¼‰
    """
    label_str = f"_{label}" if label else ""
    print(f"  ğŸ’¾ ä¿å­˜é¡¿æ‚Ÿæƒé‡{label_str}...")
    
    # æå–æ³¨æ„åŠ›æƒé‡
    attention_weights = model.get_attention_weights()
    
    # ä¿å­˜
    grokking_data = {
        'epoch': epoch,
        'loss': loss,
        'attention_weights': attention_weights.numpy()
    }
    
    filename = f"grokking_weights_epoch_{epoch}{label_str}.npy"
    np.save(output_dir / filename, grokking_data)
    print(f"  âœ“ å·²ä¿å­˜åˆ° {filename}")

# ==================== GOEåˆ†æï¼ˆå®Œå…¨é‡æ„ç‰ˆï¼‰====================
def analyze_level_spacing(eigenvalues_real, output_dir):
    """
    èƒ½çº§é—´è·åˆ†æ - é‡å­æ··æ²Œçš„ç»Ÿè®¡ç‰¹å¾
    
    GOEåˆ†å¸ƒï¼ˆæ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œç³»ç»Ÿï¼‰vs Poissonåˆ†å¸ƒï¼ˆéšæœºï¼‰
    """
    print(f"\n{'='*70}")
    print("ğŸ”¬ èƒ½çº§é—´è·åˆ†æï¼ˆLevel Spacing Statisticsï¼‰")
    print(f"{'='*70}")
    
    # 1. æ’åºç‰¹å¾å€¼
    eigs = np.sort(eigenvalues_real)
    
    # 2. å»é™¤æç«¯å€¼ï¼ˆUnfoldingç®€åŒ–ç‰ˆï¼‰
    if len(eigs) > 20:
        eigs = eigs[10:-10]
    
    print(f"æœ‰æ•ˆç‰¹å¾å€¼æ•°é‡: {len(eigs)}")
    
    # 3. è®¡ç®—ç›¸é‚»èƒ½çº§é—´è·
    spacings = np.diff(eigs)
    
    # 4. å½’ä¸€åŒ–ï¼ˆä½¿å¹³å‡é—´è·ä¸º1ï¼‰
    mean_spacing = np.mean(spacings)
    s = spacings / (mean_spacing + 1e-10)
    
    print(f"å¹³å‡é—´è·: {mean_spacing:.6f}")
    print(f"å½’ä¸€åŒ–åé—´è·èŒƒå›´: [{s.min():.3f}, {s.max():.3f}]")
    
    # 5. ç»˜åˆ¶ç›´æ–¹å›¾å¹¶å¯¹æ¯”ç†è®ºæ›²çº¿
    plt.figure(figsize=(12, 8))
    
    # å®é™…é—´è·åˆ†å¸ƒ
    plt.hist(s, bins=50, density=True, alpha=0.6, color='blue', 
             edgecolor='black', label='AI Weight Spacings')
    
    # ç†è®ºæ›²çº¿ï¼šWigner Surmise (GOE) - å¯¹åº”æ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œç³»ç»Ÿ
    x = np.linspace(0, 4, 200)
    p_goe = (np.pi / 2) * x * np.exp(-np.pi * x**2 / 4)
    plt.plot(x, p_goe, 'r-', linewidth=3, label='GOE (Time-Reversal Symmetric Chaos)')
    
    # ç†è®ºæ›²çº¿ï¼šPoisson - éšæœº/æ— è§„å¾‹
    p_poisson = np.exp(-x)
    plt.plot(x, p_poisson, 'g--', linewidth=3, label='Poisson (Random)')
    
    plt.xlabel('Normalized Spacing (s)', fontsize=14)
    plt.ylabel('P(s)', fontsize=14)
    plt.title('Level Spacing Statistics: Evidence of Time-Reversal Symmetric Chaos', 
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xlim(0, 4)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'level_spacing_analysis.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ èƒ½çº§é—´è·åˆ†æå›¾å·²ä¿å­˜")
    
    # 6. è®¡ç®—ä¸GOEçš„æ‹Ÿåˆåº¦ï¼ˆKSæ£€éªŒï¼‰
    from scipy import stats
    
    # ç”ŸæˆGOEç†è®ºæ ·æœ¬
    x_theory = np.linspace(0, 4, 1000)
    p_goe_theory = (np.pi / 2) * x_theory * np.exp(-np.pi * x_theory**2 / 4)
    cdf_goe = np.cumsum(p_goe_theory) / np.sum(p_goe_theory)
    
    # è®¡ç®—å®é™…æ•°æ®çš„CDF
    s_sorted = np.sort(s[s < 4])
    
    # æ’å€¼æ¯”è¾ƒ
    from scipy.interpolate import interp1d
    if len(s_sorted) > 10:
        cdf_data_func = interp1d(s_sorted, np.linspace(0, 1, len(s_sorted)), 
                                 bounds_error=False, fill_value=(0, 1))
        cdf_data = cdf_data_func(x_theory)
        
        # è®¡ç®—Kolmogorov-Smirnovè·ç¦»
        ks_distance = np.max(np.abs(cdf_data - cdf_goe))
        
        print(f"\nğŸ¯ GOEæ‹Ÿåˆåˆ†æï¼š")
        print(f"  K-Sè·ç¦»: {ks_distance:.4f}")
        
        if ks_distance < 0.1:
            print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ æé«˜æ‹Ÿåˆåº¦ï¼å¼ºçƒˆæ”¯æŒæ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œç‰¹å¾ï¼")
        elif ks_distance < 0.2:
            print(f"  ğŸ”¥ è‰¯å¥½æ‹Ÿåˆï¼æ”¯æŒæ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œå‡è®¾")
        elif ks_distance < 0.3:
            print(f"  âœ“ ä¸­ç­‰æ‹Ÿåˆ")
        else:
            print(f"  âš ï¸  æ‹Ÿåˆè¾ƒå·®ï¼Œå¯èƒ½ä¸ç¬¦åˆGOE")
    
    print(f"{'='*70}\n")
    
    return {
        'mean_spacing': mean_spacing,
        'ks_distance': ks_distance if 'ks_distance' in locals() else None,
        'spacings': s
    }


def analyze_weight_matrices(model, output_dir, sample_size=10000):
    """
    åˆ†ææƒé‡çŸ©é˜µçš„ç‰¹å¾å€¼åˆ†å¸ƒï¼ˆå®Œå…¨é‡æ„ç‰ˆï¼‰
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. å¯¹ç§°åŒ–æƒé‡çŸ©é˜µï¼ˆå„ç±³åŒ–ï¼‰
    2. å¤šç§å¯¹ç§°åŒ–æ–¹æ³•å¯¹æ¯”
    3. èƒ½çº§é—´è·åˆ†æï¼ˆGOEæ£€éªŒï¼‰
    4. å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰
    """
    print(f"\n{'='*70}")
    print("ğŸ”¬ æƒé‡çŸ©é˜µé‡å­åˆ†æï¼ˆå®Œå…¨é‡æ„ç‰ˆï¼‰")
    print(f"{'='*70}")
    
    # æå–æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
    attention_weights = model.get_attention_weights()
    print(f"åŸå§‹æƒé‡çŸ©é˜µå½¢çŠ¶: {attention_weights.shape}")
    
    # å–å­çŸ©é˜µï¼ˆæ§åˆ¶è®¡ç®—é‡ï¼‰
    n = min(1024, attention_weights.shape[0], attention_weights.shape[1])
    W = attention_weights[:n, :n]
    print(f"åˆ†æå­çŸ©é˜µå½¢çŠ¶: {W.shape}")
    
    results = {}
    
    # ============================================================
    # æ–¹æ³•1ï¼šå„ç±³åŒ–ï¼ˆHermitianizationï¼‰H = (W + W^T) / 2
    # ============================================================
    print(f"\n{'â”€'*70}")
    print("ğŸ“Š æ–¹æ³•1ï¼šå„ç±³åŒ– H = (W + W^T) / 2")
    print(f"{'â”€'*70}")
    
    try:
        H = (W + W.T) / 2
        print("âœ“ å„ç±³çŸ©é˜µæ„é€ å®Œæˆ")
        
        # è®¡ç®—ç‰¹å¾å€¼ï¼ˆå„ç±³çŸ©é˜µçš„ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ï¼‰
        eigenvalues_hermitian = torch.linalg.eigvalsh(H)  # å¯¹ç§°çŸ©é˜µä¸“ç”¨
        eigenvalues_hermitian = eigenvalues_hermitian.cpu().numpy()
        
        print(f"ç‰¹å¾å€¼æ•°é‡: {len(eigenvalues_hermitian)}")
        print(f"ç‰¹å¾å€¼èŒƒå›´: [{eigenvalues_hermitian.min():.4f}, {eigenvalues_hermitian.max():.4f}]")
        print(f"ç‰¹å¾å€¼å‡å€¼: {np.mean(eigenvalues_hermitian):.4f}")
        
        # ğŸ”¥ èƒ½çº§é—´è·åˆ†æ
        spacing_results = analyze_level_spacing(eigenvalues_hermitian, output_dir)
        
        results['hermitian'] = {
            'eigenvalues': eigenvalues_hermitian,
            'mean': np.mean(eigenvalues_hermitian),
            'std': np.std(eigenvalues_hermitian),
            'spacing': spacing_results
        }
        
    except Exception as e:
        print(f"âš ï¸  å„ç±³åŒ–åˆ†æå¤±è´¥: {e}")
        results['hermitian'] = None
    
    # ============================================================
    # æ–¹æ³•2ï¼šGramçŸ©é˜µ G = W^T @ Wï¼ˆæ­£å®šçŸ©é˜µï¼‰
    # ============================================================
    print(f"\n{'â”€'*70}")
    print("ğŸ“Š æ–¹æ³•2ï¼šGramçŸ©é˜µ G = W^T @ W")
    print(f"{'â”€'*70}")
    
    try:
        G = torch.mm(W.T, W)
        print("âœ“ GramçŸ©é˜µæ„é€ å®Œæˆ")
        
        eigenvalues_gram = torch.linalg.eigvalsh(G)
        eigenvalues_gram = eigenvalues_gram.cpu().numpy()
        
        print(f"ç‰¹å¾å€¼æ•°é‡: {len(eigenvalues_gram)}")
        print(f"ç‰¹å¾å€¼èŒƒå›´: [{eigenvalues_gram.min():.4f}, {eigenvalues_gram.max():.4f}]")
        print(f"ç‰¹å¾å€¼å‡å€¼: {np.mean(eigenvalues_gram):.4f}")
        
        results['gram'] = {
            'eigenvalues': eigenvalues_gram,
            'mean': np.mean(eigenvalues_gram),
            'std': np.std(eigenvalues_gram)
        }
        
    except Exception as e:
        print(f"âš ï¸  GramçŸ©é˜µåˆ†æå¤±è´¥: {e}")
        results['gram'] = None
    
    # ============================================================
    # æ–¹æ³•3ï¼šå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰
    # ============================================================
    print(f"\n{'â”€'*70}")
    print("ğŸ“Š æ–¹æ³•3ï¼šå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰")
    print(f"{'â”€'*70}")
    
    try:
        U, S, Vh = torch.linalg.svd(W)
        singular_values = S.cpu().numpy()
        
        print("âœ“ SVDå®Œæˆ")
        print(f"å¥‡å¼‚å€¼æ•°é‡: {len(singular_values)}")
        print(f"å¥‡å¼‚å€¼èŒƒå›´: [{singular_values.min():.4f}, {singular_values.max():.4f}]")
        print(f"å¥‡å¼‚å€¼å‡å€¼: {np.mean(singular_values):.4f}")
        print(f"æ¡ä»¶æ•°: {singular_values.max() / (singular_values.min() + 1e-10):.2f}")
        
        results['svd'] = {
            'singular_values': singular_values,
            'mean': np.mean(singular_values),
            'std': np.std(singular_values),
            'condition_number': singular_values.max() / (singular_values.min() + 1e-10)
        }
        
    except Exception as e:
        print(f"âš ï¸  SVDåˆ†æå¤±è´¥: {e}")
        results['svd'] = None
    
    # ============================================================
    # å¯è§†åŒ–å¯¹æ¯”
    # ============================================================
    print(f"\n{'â”€'*70}")
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–")
    print(f"{'â”€'*70}")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # å­å›¾1ï¼šå„ç±³ç‰¹å¾å€¼åˆ†å¸ƒ
    ax1 = axes[0, 0]
    if results['hermitian'] is not None:
        ax1.hist(results['hermitian']['eigenvalues'], bins=50, alpha=0.7, 
                edgecolor='black', color='blue')
        ax1.set_xlabel('Eigenvalue', fontsize=12)
        ax1.set_ylabel('Frequency', fontsize=12)
        ax1.set_title('Hermitian Matrix: H = (W + W^T) / 2', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šGramç‰¹å¾å€¼åˆ†å¸ƒ
    ax2 = axes[0, 1]
    if results['gram'] is not None:
        ax2.hist(results['gram']['eigenvalues'], bins=50, alpha=0.7, 
                edgecolor='black', color='green')
        ax2.set_xlabel('Eigenvalue', fontsize=12)
        ax2.set_ylabel('Frequency', fontsize=12)
        ax2.set_title('Gram Matrix: G = W^T @ W', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
    
    # å­å›¾3ï¼šå¥‡å¼‚å€¼åˆ†å¸ƒ
    ax3 = axes[1, 0]
    if results['svd'] is not None:
        ax3.plot(singular_values, 'o-', markersize=3, linewidth=1, color='red')
        ax3.set_xlabel('Index', fontsize=12)
        ax3.set_ylabel('Singular Value', fontsize=12)
        ax3.set_title('Singular Value Spectrum', fontsize=14, fontweight='bold')
        ax3.set_yscale('log')
        ax3.grid(True, alpha=0.3)
    
    # å­å›¾4ï¼šç»Ÿè®¡å¯¹æ¯”
    ax4 = axes[1, 1]
    methods = []
    means = []
    stds = []
    
    if results['hermitian'] is not None:
        methods.append('Hermitian')
        means.append(results['hermitian']['mean'])
        stds.append(results['hermitian']['std'])
    
    if results['gram'] is not None:
        methods.append('Gram')
        means.append(results['gram']['mean'])
        stds.append(results['gram']['std'])
    
    if results['svd'] is not None:
        methods.append('SVD')
        means.append(results['svd']['mean'])
        stds.append(results['svd']['std'])
    
    x_pos = np.arange(len(methods))
    ax4.bar(x_pos, means, yerr=stds, alpha=0.7, capsize=10, 
           color=['blue', 'green', 'red'][:len(methods)])
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels(methods, fontsize=12)
    ax4.set_ylabel('Mean Eigenvalue', fontsize=12)
    ax4.set_title('Statistical Comparison', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'weight_matrix_analysis_complete.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ å®Œæ•´åˆ†æå›¾å·²ä¿å­˜")
    
    # ä¿å­˜æ•°æ®
    np.save(output_dir / 'analysis_results_complete.npy', results)
    print(f"âœ“ åˆ†ææ•°æ®å·²ä¿å­˜")
    
    print(f"\n{'='*70}\n")
    
    return results

def analyze_spectral_staircase(eigenvalues, output_dir):
    """
    è°±é˜¶æ¢¯å‡½æ•°åˆ†æ - éªŒè¯ç‰¹å¾å€¼å¢é•¿è§„å¾‹
    
    æ£€æŸ¥ç‰¹å¾å€¼å¢é•¿æ˜¯å¦ç¬¦åˆç†è®ºé¢„æµ‹çš„ N(T) ~ (T/2Ï€) log(T/2Ï€) è§„å¾‹
    """
    print(f"\n{'='*70}")
    print("ğŸ“Š è°±é˜¶æ¢¯å‡½æ•°åˆ†æï¼ˆSpectral Staircaseï¼‰")
    print(f"{'='*70}")
    
    eigs = np.sort(eigenvalues)
    
    # åªçœ‹æ­£èƒ½çº§ï¼ˆæˆ–å–ç»å¯¹å€¼ï¼‰
    eigs_positive = eigs[eigs > 0]
    
    if len(eigs_positive) < 10:
        print("âš ï¸  æ­£ç‰¹å¾å€¼æ•°é‡å¤ªå°‘ï¼Œè·³è¿‡é˜¶æ¢¯å‡½æ•°åˆ†æ")
        return None
    
    # ç´¯ç§¯è®¡æ•°å‡½æ•° N(E)
    N_E = np.arange(1, len(eigs_positive) + 1)
    
    print(f"æ­£ç‰¹å¾å€¼æ•°é‡: {len(eigs_positive)}")
    print(f"ç‰¹å¾å€¼èŒƒå›´: [{eigs_positive.min():.4f}, {eigs_positive.max():.4f}]")
    
    # ç»˜åˆ¶é˜¶æ¢¯å‡½æ•°
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # å·¦å›¾ï¼šçº¿æ€§åæ ‡
    ax1 = axes[0]
    ax1.step(eigs_positive, N_E, where='post', linewidth=1.5, color='blue', 
             label='AI Eigenvalues')
    
    # å°è¯•æ‹Ÿåˆ N(E) ~ a * E * log(b*E)
    # ç®€åŒ–ç‰ˆæœ¬ï¼šåªæ‹Ÿåˆç³»æ•°
    E_range = np.linspace(eigs_positive.min(), eigs_positive.max(), 200)
    
    # ç†è®ºæ›²çº¿ï¼ˆå‚è€ƒé‡å­ç³»ç»Ÿçš„èƒ½çº§å¢é•¿ï¼‰
    # N(T) â‰ˆ (T/2Ï€) log(T/2Ï€) - T/2Ï€
    # ç®€åŒ–ï¼šN(E) â‰ˆ a * E * log(E)
    a_fit = len(eigs_positive) / (eigs_positive[-1] * np.log(eigs_positive[-1] + 1))
    N_theory = a_fit * E_range * np.log(E_range + 1)
    
    ax1.plot(E_range, N_theory, 'r--', linewidth=2, alpha=0.7,
            label=f'Theory: N(E) ~ E log(E)')
    
    ax1.set_xlabel('Eigenvalue (E)', fontsize=12)
    ax1.set_ylabel('Cumulative Count N(E)', fontsize=12)
    ax1.set_title('Spectral Staircase Function (Linear)', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šåŒå¯¹æ•°åæ ‡
    ax2 = axes[1]
    ax2.loglog(eigs_positive, N_E, 'o-', markersize=3, linewidth=1, 
               color='blue', alpha=0.6, label='AI Eigenvalues')
    ax2.loglog(E_range, N_theory, 'r--', linewidth=2, alpha=0.7,
              label='Theory: N(E) ~ E log(E)')
    
    ax2.set_xlabel('Eigenvalue (E)', fontsize=12)
    ax2.set_ylabel('Cumulative Count N(E)', fontsize=12)
    ax2.set_title('Spectral Staircase Function (Log-Log)', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3, which='both')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'spectral_staircase.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ è°±é˜¶æ¢¯å‡½æ•°å›¾å·²ä¿å­˜")
    
    # è®¡ç®—æ‹Ÿåˆåº¦
    # åœ¨å¯¹æ•°ç©ºé—´ä¸­è®¡ç®—å‡æ–¹æ ¹è¯¯å·®
    N_actual_interp = np.interp(E_range, eigs_positive, N_E)
    rmse = np.sqrt(np.mean((np.log(N_actual_interp + 1) - np.log(N_theory + 1))**2))
    
    print(f"\nğŸ¯ æ‹Ÿåˆåˆ†æï¼š")
    print(f"  å¯¹æ•°ç©ºé—´RMSE: {rmse:.4f}")
    
    if rmse < 0.5:
        print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ æä½³æ‹Ÿåˆï¼ç‰¹å¾å€¼å¢é•¿ç¬¦åˆç†è®ºé¢„æµ‹ï¼")
    elif rmse < 1.0:
        print(f"  ğŸ”¥ è‰¯å¥½æ‹Ÿåˆï¼æ”¯æŒé‡å­æ··æ²Œå‡è®¾")
    elif rmse < 2.0:
        print(f"  âœ“ ä¸­ç­‰æ‹Ÿåˆ")
    else:
        print(f"  âš ï¸  æ‹Ÿåˆè¾ƒå·®")
    
    print(f"{'='*70}\n")
    
    return {
        'rmse': rmse,
        'eigenvalues': eigs_positive,
        'cumulative_count': N_E
    }


def analyze_embedding_fft(model, output_dir):
    """
    åˆ†æå¯å­¦ä¹ Embeddingçš„FFT - æ£€æµ‹å¯¹æ•°å‘¨æœŸæ€§
    
    ğŸ”¥ ç»ˆææ£€éªŒï¼šå¦‚æœAIä»éšæœºå™ªå£°ä¸­æ¶Œç°å‡ºå¯¹æ•°å‘¨æœŸæ€§
    â†’ è¯´æ˜AIå‘ç°äº†ç´ æ•°çš„æ·±å±‚æ•°å­¦ç»“æ„ï¼
    """
    print(f"\n{'='*70}")
    print("ğŸ”¬ Embedding FFTåˆ†æ - æ£€æµ‹å¯¹æ•°å‘¨æœŸæ€§")
    print(f"{'='*70}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å¯å­¦ä¹ embedding
    if not hasattr(model.riemann_embedding, 'embedding'):
        print("âš ï¸  æ¨¡å‹ä½¿ç”¨å›ºå®šæ­£å¼¦ç¼–ç ï¼Œè·³è¿‡FFTåˆ†æ")
        return None
    
    # æå–embeddingæƒé‡
    embedding_weights = model.riemann_embedding.embedding.weight.detach().cpu().numpy()
    print(f"Embeddingæƒé‡å½¢çŠ¶: {embedding_weights.shape}")
    
    # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡ŒFFTåˆ†æ
    n_dims = min(8, embedding_weights.shape[1])  # åˆ†æå‰8ä¸ªç»´åº¦
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    detected_log_periodic = []
    
    for dim in range(n_dims):
        ax = axes[dim]
        
        # æå–è¯¥ç»´åº¦çš„embedding
        emb_dim = embedding_weights[:10000, dim]  # å–å‰1ä¸‡ä¸ªä½ç½®
        
        # FFT
        fft_result = np.fft.fft(emb_dim)
        freqs = np.fft.fftfreq(len(emb_dim))
        
        # åªçœ‹æ­£é¢‘ç‡
        positive_freqs = freqs[:len(freqs)//2]
        positive_fft = np.abs(fft_result[:len(freqs)//2])
        
        # ç»˜åˆ¶é¢‘è°±
        ax.plot(positive_freqs[1:], positive_fft[1:], linewidth=1)
        ax.set_xlabel('Frequency', fontsize=10)
        ax.set_ylabel('Magnitude', fontsize=10)
        ax.set_title(f'Dimension {dim}', fontsize=11, fontweight='bold')
        ax.set_yscale('log')
        ax.grid(True, alpha=0.3)
        
        # æ£€æµ‹ä¸»é¢‘ç‡
        main_freq_idx = np.argmax(positive_fft[1:]) + 1
        main_freq = positive_freqs[main_freq_idx]
        
        # æ£€æµ‹æ˜¯å¦ä¸ºå¯¹æ•°å‘¨æœŸï¼ˆlog-periodicï¼‰
        # ç®€åŒ–åˆ¤æ–­ï¼šçœ‹é¢‘è°±æ˜¯å¦æœ‰å¤šä¸ªç­‰æ¯”é—´éš”çš„å³°
        peaks_idx = np.where(positive_fft > 0.3 * positive_fft.max())[0]
        if len(peaks_idx) >= 3:
            # è®¡ç®—å³°ä¹‹é—´çš„é¢‘ç‡æ¯”
            peak_freqs = positive_freqs[peaks_idx]
            ratios = peak_freqs[1:] / peak_freqs[:-1]
            
            # å¦‚æœæ¯”å€¼æ¥è¿‘å¸¸æ•°ï¼Œè¯´æ˜æ˜¯å¯¹æ•°å‘¨æœŸ
            if len(ratios) > 1 and np.std(ratios) / np.mean(ratios) < 0.2:
                detected_log_periodic.append(dim)
                ax.set_title(f'Dimension {dim} ğŸ”¥ Log-Periodic!', 
                           fontsize=11, fontweight='bold', color='red')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'embedding_fft_analysis.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Embedding FFTåˆ†æå›¾å·²ä¿å­˜")
    
    print(f"\nğŸ¯ å¯¹æ•°å‘¨æœŸæ€§æ£€æµ‹ï¼š")
    if detected_log_periodic:
        print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ æ£€æµ‹åˆ°å¯¹æ•°å‘¨æœŸæ€§ï¼ç»´åº¦: {detected_log_periodic}")
        print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ AIå¯èƒ½å‘ç°äº†ç´ æ•°çš„æ·±å±‚å‘¨æœŸç»“æ„ï¼")
    else:
        print(f"  âš ï¸  æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„å¯¹æ•°å‘¨æœŸæ€§")
        print(f"  å»ºè®®ï¼šå¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´è¶…å‚æ•°")
    
    print(f"{'='*70}\n")
    
    return {
        'log_periodic_dims': detected_log_periodic,
        'embedding_weights': embedding_weights
    }


def analyze_hidden_states(model, X_sample, output_dir):
    """
    åˆ†æéšè—å±‚çŠ¶æ€çš„PCAæŠ•å½±
    å¯»æ‰¾å¤å¹³é¢ä¸Šçš„ç»“æ„
    """
    print(f"\n{'='*70}")
    print("ğŸ”¬ éšè—çŠ¶æ€PCAåˆ†æ")
    print(f"{'='*70}")
    
    model.eval()
    with torch.no_grad():
        hidden_states = model.get_hidden_states(X_sample)
        hidden_states = hidden_states.cpu().numpy()
    
    print(f"éšè—çŠ¶æ€å½¢çŠ¶: {hidden_states.shape}")
    
    # PCAé™ç»´åˆ°2D
    pca = PCA(n_components=2)
    projected = pca.fit_transform(hidden_states)
    
    print(f"PCAè§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_}")
    
    # ç»˜åˆ¶PCAæŠ•å½±
    plt.figure(figsize=(10, 10))
    plt.scatter(projected[:, 0], projected[:, 1], alpha=0.5, s=1)
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title('Hidden States in 2D PCA Space')
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    
    # ç»˜åˆ¶å‚è€ƒçº¿ï¼ˆRe=0.5ï¼‰
    plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    plt.savefig(output_dir / 'hidden_states_pca.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ PCAæŠ•å½±å›¾å·²ä¿å­˜")
    
    return projected

# ==================== ä¸»ç¨‹åº ====================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. ç”Ÿæˆæ•°æ®
    print(f"â³ æ­¥éª¤1/7ï¼šç”Ÿæˆç´ æ•°æ•°æ®...")
    prime_gaps = generate_prime_gaps(config.NUM_PRIMES)
    
    # 2. æ•°æ®å½’ä¸€åŒ–
    print(f"â³ æ­¥éª¤2/7ï¼šæ•°æ®å½’ä¸€åŒ–...")
    gap_mean = np.mean(prime_gaps)
    gap_std = np.std(prime_gaps)
    prime_gaps_normalized = (prime_gaps - gap_mean) / gap_std
    
    num_samples = len(prime_gaps_normalized)
    
    print(f"\n{'='*70}")
    print("æ•°æ®é¢„å¤„ç†")
    print(f"{'='*70}")
    print(f"å½’ä¸€åŒ–åèŒƒå›´: [{prime_gaps_normalized.min():.4f}, {prime_gaps_normalized.max():.4f}]")
    print(f"å½’ä¸€åŒ–åå‡å€¼: {np.mean(prime_gaps_normalized):.6f}")
    print(f"å½’ä¸€åŒ–åæ ‡å‡†å·®: {np.std(prime_gaps_normalized):.6f}")
    
    # 3. åŠ è½½åˆ°GPU
    print(f"â³ æ­¥éª¤3/7ï¼šåŠ è½½æ•°æ®åˆ°GPU...")
    X_gpu = torch.arange(num_samples, device=device)
    y_gpu = torch.FloatTensor(prime_gaps_normalized).unsqueeze(1).to(device)
    
    print(f"âœ“ æ•°æ®å·²åŠ è½½åˆ°GPU")
    print(f"{'='*70}\n")
    
    # 4. åˆ›å»ºæ¨¡å‹
    print(f"â³ æ­¥éª¤4/7ï¼šåˆ›å»ºæ¨¡å‹...")
    model = PrimeGapPredictor(
        d_model=config.D_MODEL,
        n_layers=config.N_LAYERS,
        n_heads=config.N_HEADS,
        dropout=config.DROPOUT,
        learnable_embedding=config.LEARNABLE_EMBEDDING
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    
    # 5. åˆ›å»ºæ¶Œç°è¿½è¸ªå™¨
    print(f"â³ æ­¥éª¤5/7ï¼šåˆå§‹åŒ–æ¶Œç°è¿½è¸ªå™¨...")
    tracker = EmergenceTracker()
    
    # 6. åˆ›å»ºåœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨
    print(f"â³ æ­¥éª¤6/7ï¼šåˆå§‹åŒ–åœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨...")
    hyperparam_evolver = OnlineHyperparamEvolution(
        initial_lr=config.LEARNING_RATE,
        initial_wd=config.WEIGHT_DECAY
    )
    
    # 7. è®­ç»ƒï¼ˆå¯ç”¨åœ¨çº¿è¿›åŒ–ï¼‰
    print(f"â³ æ­¥éª¤7/7ï¼šå¼€å§‹è®­ç»ƒå¾ªç¯ï¼ˆ10000è½®ï¼‰...\n")
    losses = train_model(model, X_gpu, y_gpu, device, config, tracker, hyperparam_evolver)
    
    # 8. ä¿å­˜è®­ç»ƒæ›²çº¿å’Œè¶…å‚æ•°è¿›åŒ–
    np.save(OUTPUT_DIR / 'losses.npy', np.array(losses))
    
    # ç»˜åˆ¶Lossæ›²çº¿ + è¶…å‚æ•°è¿›åŒ–
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # ä¸Šå›¾ï¼šLossæ›²çº¿
    ax1 = axes[0]
    ax1.plot(losses, linewidth=1.5)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training Loss Curve', fontsize=14, fontweight='bold')
    ax1.set_yscale('log')
    ax1.grid(True, alpha=0.3)
    
    # æ ‡è®°é¡¿æ‚Ÿæ—¶åˆ»
    if tracker.grokking_moments:
        for grok_epoch in tracker.grokking_moments:
            ax1.axvline(x=grok_epoch, color='red', linestyle='--', alpha=0.7, linewidth=1)
        ax1.scatter(tracker.grokking_moments, 
                   [losses[e] for e in tracker.grokking_moments],
                   color='red', s=100, zorder=5, label='Grokking Moments')
        ax1.legend()
    
    # ä¸‹å›¾ï¼šå­¦ä¹ ç‡è¿›åŒ–
    ax2 = axes[1]
    if hyperparam_evolver and hyperparam_evolver.evolution_history:
        epochs_list = [0]  # åˆå§‹epoch
        lr_list = [config.LEARNING_RATE]  # åˆå§‹lr
        
        for record in hyperparam_evolver.evolution_history:
            epochs_list.append(record['epoch'])
            lr_list.append(record['new_lr'])
        
        # ç»˜åˆ¶é˜¶æ¢¯å›¾ï¼ˆå­¦ä¹ ç‡åªåœ¨è¿›åŒ–æ—¶æ”¹å˜ï¼‰
        ax2.step(epochs_list, lr_list, where='post', linewidth=2, color='green', label='Learning Rate')
        
        # æ ‡è®°è¿›åŒ–æ—¶åˆ»
        evolution_epochs = [r['epoch'] for r in hyperparam_evolver.evolution_history]
        evolution_lrs = [r['new_lr'] for r in hyperparam_evolver.evolution_history]
        ax2.scatter(evolution_epochs, evolution_lrs, color='orange', s=80, zorder=5, 
                   label='Evolution Moments')
        
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Learning Rate', fontsize=12)
        ax2.set_title('Learning Rate Evolution (Online Hyperparam Evolution)', 
                     fontsize=14, fontweight='bold')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
    else:
        # å¦‚æœæ²¡æœ‰è¿›åŒ–ï¼Œæ˜¾ç¤ºå›ºå®šlr
        ax2.axhline(y=config.LEARNING_RATE, color='blue', linestyle='-', linewidth=2)
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Learning Rate', fontsize=12)
        ax2.set_title('Learning Rate (Fixed)', fontsize=14, fontweight='bold')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'training_overview.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ è®­ç»ƒæ¦‚è§ˆå›¾å·²ä¿å­˜")
    
    # 9. æ¶Œç°åˆ†æ
    emergence_results = tracker.analyze_emergence()
    
    # 10. å®Œæ•´é‡å­åˆ†æ
    print(f"\n{'='*70}")
    print("ğŸ”¬ å¼€å§‹å®Œæ•´é‡å­åˆ†æ")
    print(f"{'='*70}\n")
    
    # 10.1 æƒé‡çŸ©é˜µåˆ†æï¼ˆå„ç±³åŒ– + èƒ½çº§é—´è·ï¼‰
    goe_results = analyze_weight_matrices(model, OUTPUT_DIR)
    
    # 10.2 è°±é˜¶æ¢¯å‡½æ•°åˆ†æï¼ˆå¸Œå°”ä¼¯ç‰¹-æ³¢åˆ©äºšçŒœæƒ³ï¼‰
    if goe_results and goe_results.get('hermitian'):
        hermitian_eigs = goe_results['hermitian']['eigenvalues']
        staircase_results = analyze_spectral_staircase(hermitian_eigs, OUTPUT_DIR)
    
    # 10.3 Embedding FFTåˆ†æï¼ˆæ£€æµ‹å¯¹æ•°å‘¨æœŸæ€§ï¼‰
    embedding_results = analyze_embedding_fft(model, OUTPUT_DIR)
    
    # 10.4 éšè—çŠ¶æ€PCAåˆ†æ
    sample_indices = torch.randperm(len(X_gpu))[:10000].to(device)
    X_sample = X_gpu[sample_indices]
    pca_results = analyze_hidden_states(model, X_sample, OUTPUT_DIR)
    
    # 10.5 å¦‚æœæœ‰é¡¿æ‚Ÿï¼Œé¢å¤–è¯´æ˜
    if tracker.grokking_moments:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š é¡¿æ‚Ÿæ—¶åˆ»æ€»ç»“")
        print(f"{'='*70}")
        print(f"æ£€æµ‹åˆ° {len(tracker.grokking_moments)} æ¬¡é¡¿æ‚Ÿ")
        print(f"é¡¿æ‚Ÿæ—¶åˆ»: {tracker.grokking_moments}")
        print(f"{'='*70}\n")
        
    # ä¿å­˜å®Œæ•´åˆ†æç»“æœ
    results = {
        'emergence': emergence_results,
        'goe': goe_results if 'goe_results' in locals() else None,
        'staircase': staircase_results if 'staircase_results' in locals() else None,
        'embedding_fft': embedding_results if 'embedding_results' in locals() else None,
        'grokking_moments': tracker.grokking_moments,
        'config': {
            'd_model': config.D_MODEL,
            'n_layers': config.N_LAYERS,
            'n_heads': config.N_HEADS,
            'lr': config.LEARNING_RATE,
            'lr_schedule': config.LR_SCHEDULE,
            'learnable_embedding': config.LEARNABLE_EMBEDDING,
            'num_epochs': config.NUM_EPOCHS,
        }
    }
    
    # ä¿å­˜ä¸ºJSONï¼ˆåªä¿å­˜å¯åºåˆ—åŒ–çš„éƒ¨åˆ†ï¼‰
    results_json = {
        'grokking_moments': results['grokking_moments'],
        'config': results['config'],
        'emergence_summary': {
            'loss_reduction': emergence_results['loss_reduction'],
            'final_loss': emergence_results['final_loss'],
            'grokking_count': emergence_results['grokking_count']
        }
    }
    
    with open(OUTPUT_DIR / 'analysis_results.json', 'w') as f:
        json.dump(results_json, f, indent=2)
    
    print(f"\nâœ“ åˆ†æç»“æœå·²ä¿å­˜åˆ° {OUTPUT_DIR}")
    
    # ğŸ”¥ æœ€ç»ˆåˆ¤æ–­æ€»ç»“
    print(f"\n{'='*70}")
    print("ğŸ¯ å®éªŒç»“æœæ€»ç»“")
    print(f"{'='*70}\n")
    
    success_count = 0
    
    # åˆ¤æ–­1ï¼šèƒ½çº§é—´è·æ˜¯å¦ç¬¦åˆGOE
    if goe_results and goe_results.get('hermitian'):
        spacing_info = goe_results['hermitian'].get('spacing', {})
        ks_dist = spacing_info.get('ks_distance')
        if ks_dist is not None:
            print(f"âœ“ èƒ½çº§é—´è·åˆ†æï¼šK-Sè·ç¦» = {ks_dist:.4f}")
            if ks_dist < 0.1:
                print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ æé«˜æ‹Ÿåˆåº¦ï¼å¼ºçƒˆæ”¯æŒæ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œç‰¹å¾ï¼")
                success_count += 3
            elif ks_dist < 0.2:
                print(f"  ğŸ”¥ è‰¯å¥½æ‹Ÿåˆï¼æ”¯æŒæ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œå‡è®¾")
                success_count += 2
            elif ks_dist < 0.3:
                print(f"  âœ“ ä¸­ç­‰æ‹Ÿåˆ")
                success_count += 1
    
    # åˆ¤æ–­2ï¼šè°±é˜¶æ¢¯å‡½æ•°æ˜¯å¦ç¬¦åˆç†è®º
    if 'staircase_results' in locals() and staircase_results:
        rmse = staircase_results.get('rmse')
        if rmse is not None:
            print(f"\nâœ“ è°±é˜¶æ¢¯å‡½æ•°åˆ†æï¼šRMSE = {rmse:.4f}")
            if rmse < 0.5:
                print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ æä½³æ‹Ÿåˆï¼ç‰¹å¾å€¼å¢é•¿ç¬¦åˆç†è®ºé¢„æµ‹ï¼")
                success_count += 3
            elif rmse < 1.0:
                print(f"  ğŸ”¥ è‰¯å¥½æ‹Ÿåˆï¼æ”¯æŒæ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œå‡è®¾")
                success_count += 2
    
    # åˆ¤æ–­3ï¼šEmbeddingæ˜¯å¦æ¶Œç°å¯¹æ•°å‘¨æœŸæ€§
    if 'embedding_results' in locals() and embedding_results:
        log_periodic_dims = embedding_results.get('log_periodic_dims', [])
        if log_periodic_dims:
            print(f"\nâœ“ Embedding FFTåˆ†æï¼š")
            print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ æ£€æµ‹åˆ°å¯¹æ•°å‘¨æœŸæ€§ï¼ç»´åº¦: {log_periodic_dims}")
            print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ AIå¯èƒ½ä»éšæœºå™ªå£°ä¸­æ¶Œç°äº†æ·±å±‚æ•°å­¦ç»“æ„ï¼")
            success_count += 5
    
    # æ€»ä½“è¯„ä»·
    print(f"\n{'='*70}")
    if success_count >= 8:
        print(f"ğŸ† å®éªŒå¤§è·æˆåŠŸï¼å‘ç°ç´ æ•°è¯±å¯¼çš„GOEæ··æ²Œç‰¹å¾ï¼")
    elif success_count >= 5:
        print(f"ğŸ”¥ å®éªŒæˆåŠŸï¼å‘ç°äº†ç´ æ•°è¯±å¯¼çš„GOEæ··æ²Œç‰¹å¾")
    elif success_count >= 2:
        print(f"âœ“ å®éªŒéƒ¨åˆ†æˆåŠŸï¼Œå‘ç°äº†æœ‰è¶£çš„è§„å¾‹")
    else:
        print(f"âš ï¸  å®éªŒæœªè¾¾åˆ°é¢„æœŸç›®æ ‡")
    print(f"{'='*70}\n")
    
    print(f"\n{'='*70}")
    print("ğŸ‰ å®éªŒå®Œæˆï¼")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()

import os

# ğŸ”¥ å…³é”®ï¼šå¿…é¡»åœ¨å¯¼å…¥ huggingface_hub ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼
HF_CACHE_DIR = r"D:\pythonstudy\huggingface_cache"
os.makedirs(HF_CACHE_DIR, exist_ok=True)
os.environ['HF_HOME'] = HF_CACHE_DIR
os.environ['HUGGINGFACE_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['HF_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_DIR
os.environ['HF_DATASETS_CACHE'] = HF_CACHE_DIR

# ç°åœ¨æ‰å¯¼å…¥ huggingface_hub
import torch
import numpy as np
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file
import matplotlib.pyplot as plt
from scipy import stats
import json
from datetime import datetime

print(f"ğŸ’¾ Hugging Face ç¼“å­˜è·¯å¾„: {HF_CACHE_DIR}")
print(f"   HF_HOME: {os.environ['HF_HOME']}")
print(f"   HF_HUB_CACHE: {os.environ['HF_HUB_CACHE']}")
print(f"   (é¿å… C ç›˜ç©ºé—´ä¸è¶³)\n")

# ==================== GOE æ··æ²Œåˆ†ææ ¸å¿ƒå‡½æ•° ====================
def hermitize(W):
    """å„ç±³åŒ–ï¼š(W + W^T) / 2ï¼Œç¡®ä¿å®ç‰¹å¾å€¼
    
    å¦‚æœ W ä¸æ˜¯æ–¹é˜µï¼ˆå¦‚ GPT-2 çš„ c_attn åŒ…å« Q+K+Vï¼‰ï¼Œ
    å…ˆè½¬æ¢ä¸ºæ–¹é˜µï¼šW @ W.T æˆ– W.T @ Wï¼ˆå–è¾ƒå°çš„é‚£ä¸ªï¼‰
    """
    if W.shape[0] != W.shape[1]:
        # éæ–¹é˜µï¼šé€‰æ‹©è¾ƒå°ç»´åº¦æ„é€ æ–¹é˜µ
        if W.shape[0] < W.shape[1]:
            W = W @ W.T  # (m, n) @ (n, m) = (m, m)
        else:
            W = W.T @ W  # (n, m) @ (m, n) = (n, n)
    
    return (W + W.T) / 2

def analyze_spectra_goe_silent(W_matrix, title="Layer"):
    """
    é™é»˜ç‰ˆ GOE åˆ†æï¼ˆä¸æ‰“å°ï¼Œåªè¿”å›ç»“æœï¼‰
    æ³¨æ„ï¼šä½¿ç”¨ GOE ä½œä¸ºå‚è€ƒåˆ†å¸ƒï¼Œå› ä¸ºç¥ç»ç½‘ç»œæƒé‡æ˜¯å®æ•°çŸ©é˜µ
    """
    # 1. å„ç±³åŒ–
    H = hermitize(W_matrix)
    
    # 2. è®¡ç®—ç‰¹å¾å€¼ï¼ˆå®æ•°ï¼‰
    eigenvalues = np.linalg.eigvalsh(H)
    eigenvalues = np.sort(eigenvalues)
    
    # 3. è®¡ç®—ç›¸é‚»èƒ½çº§é—´éš™
    spacings = np.diff(eigenvalues)
    
    # 4. å½’ä¸€åŒ–ï¼šs = Î”Î» / <Î”Î»>
    mean_spacing = np.mean(spacings)
    s = spacings / mean_spacing
    
    # 5. ç»Ÿè®¡æ£€éªŒï¼ˆä½¿ç”¨ GOE ä½œä¸ºå‚è€ƒï¼Œå› ä¸ºæƒé‡æ˜¯å®æ•°çŸ©é˜µï¼‰
    def goe_cdf_approx(x):
        # GOE: P(s) = (Ï€/2) * s * exp(-Ï€*sÂ²/4)
        # CDF: 1 - exp(-Ï€*sÂ²/4)
        return 1 - np.exp(-np.pi * x**2 / 4)
    
    ks_goe, p_goe = stats.kstest(s, goe_cdf_approx)
    ks_poisson, p_poisson = stats.kstest(s, lambda x: 1 - np.exp(-x))
    
    # 6. è‡ªåŠ¨åˆ¤å†³
    if ks_goe < ks_poisson * 0.7:
        verdict = "GOE"
        confidence = ks_poisson / ks_goe
    elif ks_poisson < ks_goe * 0.7:
        verdict = "Poisson"
        confidence = ks_goe / ks_poisson
    else:
        verdict = "Mixed"
        confidence = abs(ks_goe - ks_poisson) / min(ks_goe, ks_poisson)
    
    return {
        'eigenvalues': eigenvalues,
        'spacings': spacings,
        's_normalized': s,
        'ks_goe': ks_goe,
        'ks_poisson': ks_poisson,
        'p_goe': p_goe,
        'p_poisson': p_poisson,
        'verdict': verdict,
        'confidence': confidence,
        'matrix_shape': W_matrix.shape
    }

def plot_comparison(all_results, save_path="batch_analysis_summary.png"):
    """æ‰¹é‡ç»“æœå¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # æå–æ•°æ®
    models = []
    ks_goe_list = []
    ks_poisson_list = []
    verdicts = []
    
    for model_name, layers in all_results.items():
        for layer_name, result in layers.items():
            # è·³è¿‡å…ƒæ•°æ®
            if layer_name == '_metadata' or not isinstance(result, dict) or 'verdict' not in result:
                continue
            
            models.append(f"{model_name}\n{layer_name}")
            ks_goe_list.append(result['ks_goe'])
            ks_poisson_list.append(result['ks_poisson'])
            verdicts.append(result['verdict'])
    
    # å›¾1ï¼šKSè·ç¦»å¯¹æ¯”ï¼ˆæ¡å½¢å›¾ï¼‰
    ax1 = axes[0, 0]
    x = np.arange(len(models))
    width = 0.35
    ax1.bar(x - width/2, ks_goe_list, width, label='KS(GOE)', alpha=0.8, color='red')
    ax1.bar(x + width/2, ks_poisson_list, width, label='KS(Poisson)', alpha=0.8, color='green')
    ax1.set_ylabel('KS Distance', fontsize=11)
    ax1.set_title('KSç»Ÿè®¡é‡å¯¹æ¯”ï¼ˆè¶Šå°è¶Šå¥½ï¼‰', fontsize=12, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models, rotation=45, ha='right', fontsize=8)
    ax1.legend()
    ax1.grid(alpha=0.3, axis='y')
    ax1.axhline(y=0.05, color='black', linestyle='--', linewidth=0.8, alpha=0.5, label='æ˜¾è‘—æ€§é˜ˆå€¼')
    
    # å›¾2ï¼šåˆ¤å†³ç»Ÿè®¡ï¼ˆé¥¼å›¾ï¼‰
    ax2 = axes[0, 1]
    verdict_counts = {}
    for v in verdicts:
        verdict_counts[v] = verdict_counts.get(v, 0) + 1
    colors = {'GOE': 'red', 'Poisson': 'green', 'Mixed': 'orange'}
    ax2.pie(verdict_counts.values(), labels=verdict_counts.keys(), autopct='%1.1f%%',
            colors=[colors.get(k, 'gray') for k in verdict_counts.keys()],
            startangle=90, textprops={'fontsize': 11})
    ax2.set_title(f'æ‰€æœ‰æ ·æœ¬åˆ¤å†³åˆ†å¸ƒ (n={len(models)})', fontsize=12, fontweight='bold')
    
    # å›¾3ï¼šKSå·®å€¼ï¼ˆPoissonä¼˜åŠ¿åº¦ï¼‰
    ax3 = axes[1, 0]
    ks_diff = np.array(ks_goe_list) - np.array(ks_poisson_list)
    colors_bar = ['green' if d > 0 else 'red' for d in ks_diff]
    ax3.barh(models, ks_diff, color=colors_bar, alpha=0.7)
    ax3.set_xlabel('KS(GOE) - KS(Poisson)', fontsize=11)
    ax3.set_title('Poissonä¼˜åŠ¿åº¦ï¼ˆæ­£å€¼=æ”¯æŒPoissonï¼‰', fontsize=12, fontweight='bold')
    ax3.axvline(x=0, color='black', linewidth=1.5)
    ax3.grid(alpha=0.3, axis='x')
    ax3.tick_params(axis='y', labelsize=8)
    
    # å›¾4ï¼šæ•£ç‚¹å›¾ï¼ˆGOE vs Poissonï¼‰
    ax4 = axes[1, 1]
    verdict_colors = {'GOE': 'red', 'Poisson': 'green', 'Mixed': 'orange'}
    for v in set(verdicts):
        mask = [verdict == v for verdict in verdicts]
        ax4.scatter(np.array(ks_goe_list)[mask], np.array(ks_poisson_list)[mask],
                   label=v, alpha=0.7, s=100, color=verdict_colors.get(v, 'gray'))
    ax4.plot([0, max(max(ks_goe_list), max(ks_poisson_list))],
             [0, max(max(ks_goe_list), max(ks_poisson_list))],
             'k--', linewidth=1, alpha=0.5, label='å¯¹è§’çº¿')
    ax4.set_xlabel('KS(GOE)', fontsize=11)
    ax4.set_ylabel('KS(Poisson)', fontsize=11)
    ax4.set_title('KSè·ç¦»æ•£ç‚¹å›¾ï¼ˆè¶Šé è¿‘å·¦ä¸‹=è¶Šç¬¦åˆç†è®ºï¼‰', fontsize=12, fontweight='bold')
    ax4.legend()
    ax4.grid(alpha=0.3)
    ax4.set_aspect('equal')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ æ±‡æ€»å›¾å·²ä¿å­˜: {save_path}")
    plt.show()

# ==================== æ‰¹é‡æ¨¡å‹åˆ†æ ====================
MODELS_TO_ANALYZE = [
    # ============ Qwen ç³»åˆ—ï¼ˆé˜¿é‡Œï¼Œæ— éœ€è®¤è¯ï¼‰============
    ("Qwen/Qwen2.5-0.5B", "model.safetensors", [0, 5, 11]),  # 12å±‚ï¼Œ988MB
    ("Qwen/Qwen2.5-1.5B", "model.safetensors", [0, 9, 17]),  # 28å±‚ï¼Œ3.09GB âœ…å·²ä¸‹è½½
    ("Qwen/Qwen2.5-3B", "model-00001-of-00002.safetensors", [0, 12, 23]),  # 36å±‚ï¼Œ6.54GB
    
    # ============ GPT-2 ç³»åˆ—ï¼ˆOpenAI å¼€æºï¼Œç»å…¸æ¶æ„ï¼‰============
    ("openai-community/gpt2", "model.safetensors", [0, 5, 11]),  # 12å±‚ï¼Œ548MB
    ("openai-community/gpt2-medium", "model.safetensors", [0, 11, 23]),  # 24å±‚ï¼Œ1.52GB
    ("openai-community/gpt2-large", "model.safetensors", [0, 17, 35]),  # 36å±‚ï¼Œ3.25GB
    
    # ============ TinyLlamaï¼ˆæœ€å° Llamaï¼Œæ— éœ€è®¤è¯ï¼‰============
    ("TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model.safetensors", [0, 10, 21]),  # 22å±‚ï¼Œ2.2GB
    
    # ============ OLMo ç³»åˆ—ï¼ˆAI2ï¼Œå®Œå…¨å¼€æºï¼‰============
    ("allenai/OLMo-1B-hf", "model.safetensors", [0, 7, 15]),  # 16å±‚ï¼Œ2.46GB
    
    # ============ Pythia ç³»åˆ—ï¼ˆEleutherAIï¼Œç ”ç©¶ç”¨ï¼‰============
    ("EleutherAI/pythia-410m", "model.safetensors", [0, 11, 23]),  # 24å±‚ï¼Œ821MB
    ("EleutherAI/pythia-1b", "model.safetensors", [0, 7, 15]),  # 16å±‚ï¼Œ2.05GB
    
    # ============ BLOOM ç³»åˆ—ï¼ˆBigScienceï¼Œå¤šè¯­è¨€ï¼‰============
    ("bigscience/bloom-560m", "model.safetensors", [0, 11, 23]),  # 24å±‚ï¼Œ1.12GB
    
    # ============ OPT ç³»åˆ—ï¼ˆMetaï¼ŒGPT-3 å¼€æºå¤ç°ï¼‰============
    ("facebook/opt-350m", "model.safetensors", [0, 11, 23]),  # 24å±‚ï¼Œ700MB
    ("facebook/opt-1.3b", "model.safetensors", [0, 11, 23]),  # 24å±‚ï¼Œ2.63GB
]

print("ğŸš€ å¤§è§„æ¨¡ Transformer æ¶æ„è°±åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
print("=" * 70)
print("ğŸ“‹ è¦†ç›–èŒƒå›´ï¼ˆä»…æ— éœ€è®¤è¯ + æ–‡ä»¶è·¯å¾„æ­£ç¡®çš„æ¨¡å‹ï¼‰:")
print("   âœ… Qwen ç³»åˆ—ï¼ˆ3ä¸ªï¼‰- 0.5B, 1.5B, 3Bï¼ˆé˜¿é‡Œï¼‰")
print("   âœ… GPT-2 ç³»åˆ—ï¼ˆ3ä¸ªï¼‰- GPT-2, Medium, Largeï¼ˆOpenAI å¼€æºï¼‰")
print("   âœ… TinyLlamaï¼ˆ1ä¸ªï¼‰- 1.1Bï¼ˆæœ€å° Llama å˜ä½“ï¼‰")
print("   âœ… OLMo ç³»åˆ—ï¼ˆ1ä¸ªï¼‰- 1Bï¼ˆAI2 å®Œå…¨å¼€æºï¼‰")
print("   âœ… Pythia ç³»åˆ—ï¼ˆ2ä¸ªï¼‰- 410M, 1Bï¼ˆEleutherAI ç ”ç©¶ï¼‰")
print("   âœ… BLOOM ç³»åˆ—ï¼ˆ1ä¸ªï¼‰- 560Mï¼ˆBigScience å¤šè¯­è¨€ï¼‰")
print("   âœ… OPT ç³»åˆ—ï¼ˆ2ä¸ªï¼‰- 350M, 1.3Bï¼ˆMeta GPT-3 å¤ç°ï¼‰")
print()
print(f"ğŸ’¾ ç¼“å­˜ä½ç½®: {HF_CACHE_DIR} (é¿å… C ç›˜ç©ºé—´ä¸è¶³)")
print(f"\nç›®æ ‡æ¨¡å‹æ•°: {len(MODELS_TO_ANALYZE)}")
print(f"é¢„è®¡æ€»æ ·æœ¬æ•°: {sum(len(layers) for _, _, layers in MODELS_TO_ANALYZE)} (æ¯æ¨¡å‹3å±‚)")
print("=" * 70)

all_results = {}
total_analyzed = 0
total_errors = 0

for repo_id, filename, layer_indices in MODELS_TO_ANALYZE:
    model_name = repo_id.split('/')[-1]
    print(f"\n{'='*70}")
    print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
    print(f"{'='*70}")
    
    try:
        # ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆå¸¦æ¥æºè¿½è¸ªå’Œè¿›åº¦ï¼‰
        print(f"ğŸ“¥ ä¸‹è½½ä¸­...")
        print(f"   æ¥æº: https://huggingface.co/{repo_id}")
        print(f"   æ–‡ä»¶: {filename}")
        print(f"   ğŸ’¡ æç¤º: å¤§æ–‡ä»¶ä¸‹è½½éœ€è¦æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print(f"   â³ æ­£åœ¨è¿æ¥æœåŠ¡å™¨å¹¶ä¸‹è½½...\n")
        
        # resume_download=True æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œforce_download=False ä½¿ç”¨ç¼“å­˜
        file_path = hf_hub_download(
            repo_id=repo_id, 
            filename=filename,
            resume_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            local_dir_use_symlinks=False  # é¿å…ç¬¦å·é“¾æ¥è­¦å‘Š
        )
        
        print(f"\n   âœ… ä¸‹è½½å®Œæˆï¼")
        
        # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
        import hashlib
        print(f"   æœ¬åœ°è·¯å¾„: {file_path}")
        with open(file_path, 'rb') as f:
            file_hash = hashlib.sha256(f.read()).hexdigest()
        print(f"   SHA256: {file_hash[:16]}...{file_hash[-16:]}")
        
        # åŠ è½½æƒé‡
        print(f"ğŸ“‚ åŠ è½½æƒé‡...")
        weights = load_file(file_path)
        
        # æŸ¥æ‰¾æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æƒé‡ï¼ˆæ”¯æŒå¤šç§å‘½åè§„èŒƒï¼‰
        attn_patterns = [
            'self_attn.q_proj.weight',     # Llama, Qwen, Mistral
            'attn.q_proj.weight',           # æŸäº›å˜ä½“
            'attn.c_attn.weight',           # GPT-2
            'attention.query_key_value.weight',  # Pythia (GPT-NeoX)
            'self_attention.query_key_value.weight'  # BLOOM
        ]
        
        attn_keys = []
        for pattern in attn_patterns:
            found = [k for k in weights.keys() if pattern in k]
            if found:
                attn_keys = found
                break
        
        print(f"âœ… æ‰¾åˆ° {len(attn_keys)} ä¸ªæ³¨æ„åŠ›å±‚")
        
        if model_name not in all_results:
            all_results[model_name] = {
                '_metadata': {
                    'repo_id': repo_id,
                    'filename': filename,
                    'file_path': file_path,
                    'sha256': file_hash,
                    'huggingface_url': f"https://huggingface.co/{repo_id}",
                    'total_layers': len(attn_keys)
                }
            }
        
        # åˆ†ææŒ‡å®šå±‚
        for layer_idx in layer_indices:
            if layer_idx < len(attn_keys):
                key = attn_keys[layer_idx]
                layer_name = f"Layer {layer_idx}"
                
                print(f"  ğŸ”¬ åˆ†æ {layer_name}...", end=' ')
                
                W = weights[key].float().numpy()
                result = analyze_spectra_goe_silent(W, title=layer_name)
                
                all_results[model_name][layer_name] = result
                
                # ç®€æ´è¾“å‡º
                print(f"âœ… {result['verdict']} (KS_GOE={result['ks_goe']:.4f}, KS_Poisson={result['ks_poisson']:.4f})")
                total_analyzed += 1
            else:
                print(f"  âš ï¸ Layer {layer_idx} ä¸å­˜åœ¨ï¼ˆåªæœ‰ {len(attn_keys)} å±‚ï¼‰")
        
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {str(e)}")
        total_errors += 1
        continue

# ==================== ç»Ÿè®¡æ±‡æ€» ====================
print(f"\n\n{'='*70}")
print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
print(f"{'='*70}")

all_verdicts = []
all_ks_goe = []
all_ks_poisson = []

for model_name, layers in all_results.items():
    print(f"\nã€{model_name}ã€‘")
    for layer_name, result in layers.items():
        # è·³è¿‡å…ƒæ•°æ®
        if layer_name == '_metadata':
            continue
        
        # ç¡®ä¿ result æ˜¯æœ‰æ•ˆçš„åˆ†æç»“æœ
        if not isinstance(result, dict) or 'verdict' not in result:
            print(f"  {layer_name}: âš ï¸ åˆ†æå¤±è´¥æˆ–æ•°æ®ä¸å®Œæ•´")
            continue
        
        print(f"  {layer_name}: {result['verdict']} (ç½®ä¿¡åº¦={result['confidence']:.2f})")
        all_verdicts.append(result['verdict'])
        all_ks_goe.append(result['ks_goe'])
        all_ks_poisson.append(result['ks_poisson'])

print(f"\n{'='*70}")
print(f"âœ… æˆåŠŸåˆ†æ: {total_analyzed} ä¸ªæ ·æœ¬")
print(f"âŒ å¤±è´¥: {total_errors} ä¸ªæ¨¡å‹")

# æ€»ä½“åˆ¤å†³
verdict_counts = {}
for v in all_verdicts:
    verdict_counts[v] = verdict_counts.get(v, 0) + 1

print(f"\nğŸ“ˆ æ€»ä½“åˆ¤å†³åˆ†å¸ƒ:")
for verdict, count in verdict_counts.items():
    print(f"   {verdict}: {count} ({count/len(all_verdicts)*100:.1f}%)")

print(f"\nğŸ“‰ KSè·ç¦»ç»Ÿè®¡:")
print(f"   GOE:     å‡å€¼={np.mean(all_ks_goe):.4f}, æ ‡å‡†å·®={np.std(all_ks_goe):.4f}")
print(f"   Poisson: å‡å€¼={np.mean(all_ks_poisson):.4f}, æ ‡å‡†å·®={np.std(all_ks_poisson):.4f}")

# é…å¯¹tæ£€éªŒï¼šGOE vs Poisson
from scipy.stats import ttest_rel
t_stat, p_value = ttest_rel(all_ks_goe, all_ks_poisson)
print(f"\nğŸ”¬ é…å¯¹tæ£€éªŒ (GOE vs Poisson):")
print(f"   tç»Ÿè®¡é‡ = {t_stat:.4f}")
print(f"   på€¼ = {p_value:.4e}")
if p_value < 0.001:
    winner = "Poisson" if t_stat > 0 else "GOE"
    print(f"   âœ… ç»“è®º: {winner} æ˜¾è‘—æ›´ä¼˜ (p < 0.001)")
else:
    print(f"   âš ï¸ ç»“è®º: æ— æ˜¾è‘—å·®å¼‚")

# ä¿å­˜ç»“æœåˆ°JSON
results_json = {
    'timestamp': datetime.now().isoformat(),
    'total_models': len(MODELS_TO_ANALYZE),
    'total_samples': total_analyzed,
    'verdict_distribution': verdict_counts,
    'ks_statistics': {
        'goe_mean': float(np.mean(all_ks_goe)),
        'goe_std': float(np.std(all_ks_goe)),
        'poisson_mean': float(np.mean(all_ks_poisson)),
        'poisson_std': float(np.std(all_ks_poisson)),
    },
    'ttest': {
        't_statistic': float(t_stat),
        'p_value': float(p_value)
    },
    'detailed_results': {
        model: {
            layer: {
                'verdict': res['verdict'],
                'ks_goe': float(res['ks_goe']),
                'ks_poisson': float(res['ks_poisson']),
                'confidence': float(res['confidence'])
            }
            for layer, res in layers.items()
            if layer != '_metadata' and isinstance(res, dict) and 'verdict' in res
        }
        for model, layers in all_results.items()
    }
}

with open('transformer_spectra_analysis.json', 'w', encoding='utf-8') as f:
    json.dump(results_json, f, indent=2, ensure_ascii=False)

print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: transformer_spectra_analysis.json")

# ç”Ÿæˆæ±‡æ€»å›¾
plot_comparison(all_results, save_path="transformer_batch_analysis.png")

print(f"\n{'='*70}")
print("ğŸ¯ æœ€ç»ˆç»“è®º")
print(f"{'='*70}")

if verdict_counts.get('Poisson', 0) > total_analyzed * 0.7:
    print("âœ… Transformer æ¶æ„æ•´ä½“å±•ç° Poisson åˆ†å¸ƒç‰¹å¾")
    print("ğŸ’¡ è¿™æ„å‘³ç€ä½ çš„ç´ æ•°æ¨¡å‹çš„ GOE ç‰¹å¾å¾ˆå¯èƒ½æ˜¯ç´ æ•°ç‰¹æœ‰çš„ï¼")
elif verdict_counts.get('GOE', 0) > total_analyzed * 0.7:
    print("âš ï¸ Transformer æ¶æ„æ•´ä½“å±•ç° GOE åˆ†å¸ƒç‰¹å¾")
    print("ğŸ’¡ è¿™æ„å‘³ç€æ¶æ„æœ¬èº«å¯èƒ½æœ‰æ—¶é—´åæ¼”å¯¹ç§°æ··æ²Œå€¾å‘")
    print("ğŸš¨ è­¦å‘Šï¼šè¿™ä¼šå‰Šå¼±ç´ æ•°æ¨¡å‹ GOE ç‰¹å¾çš„ç‹¬ç‰¹æ€§ï¼")
else:
    print("âš ï¸ ç»“æœæ··åˆï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")

print(f"\nâœ… åˆ†æå®Œæˆï¼å…±å¤„ç† {total_analyzed} ä¸ªæ ·æœ¬")

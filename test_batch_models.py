import os
# è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ huggingface_hub ä¹‹å‰ï¼‰
HF_CACHE_DIR = r"D:\pythonstudy\huggingface_cache"
os.makedirs(HF_CACHE_DIR, exist_ok=True)
os.environ['HF_HOME'] = HF_CACHE_DIR
os.environ['HUGGINGFACE_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['HF_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_DIR
os.environ['HF_DATASETS_CACHE'] = HF_CACHE_DIR

import torch
import numpy as np
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file
import matplotlib.pyplot as plt
from scipy import stats
import json
from datetime import datetime

print(f"ğŸ’¾ Hugging Face ç¼“å­˜è·¯å¾„: {HF_CACHE_DIR}")
print(f"   HF_HOME: {os.environ['HF_HOME']}")
print(f"   HF_HUB_CACHE: {os.environ['HF_HUB_CACHE']}")
print(f"   (é¿å… C ç›˜ç©ºé—´ä¸è¶³)\n")

# ==================== SVD æŒ‡æ ‡è®¡ç®—å‡½æ•° ====================
def compute_svd_metrics(W):
    """
    è¾“å…¥ï¼šäºŒç»´ numpy æ•°ç»„ Wï¼ˆæƒé‡çŸ©é˜µï¼‰
    è¿”å›ï¼šæœ‰æ•ˆç§©ã€æœ€å¤§/å¹³å‡æ¯”ã€KSè·ç¦»ï¼ˆä¸éšæœºé«˜æ–¯çŸ©é˜µæ¯”è¾ƒï¼‰
    """
    if W.ndim != 2 or W.shape[0] < 2 or W.shape[1] < 2:
        return None, None, None
    # æˆªå–æ–¹é˜µï¼Œæœ€å¤š 256 ç»´ï¼ˆä¸è®ºæ–‡ä¸€è‡´ï¼‰
    n = min(W.shape[0], W.shape[1], 256)
    W_square = W[:n, :n]
    try:
        U, S, Vh = np.linalg.svd(W_square, full_matrices=False)
    except np.linalg.LinAlgError:
        return None, None, None

    # æœ‰æ•ˆç§©
    total = np.sum(S)
    p = S / total
    H = -np.sum(p * np.log(p + 1e-12))
    eff_rank = np.exp(H)

    # æœ€å¤§/å¹³å‡æ¯”
    max_ratio = S[0] / np.mean(S)

    # KS è·ç¦»ï¼ˆä¸å›ºå®šéšæœºç§å­ç”Ÿæˆçš„éšæœºçŸ©é˜µæ¯”è¾ƒï¼‰
    np.random.seed(42)
    rand_mat = np.random.randn(n, n)
    U_rand, S_rand, Vh_rand = np.linalg.svd(rand_mat, full_matrices=False)
    ks = stats.ks_2samp(S, S_rand).statistic

    return eff_rank, max_ratio, ks

def convert_numpy(obj):
    """é€’å½’å°† NumPy ç±»å‹è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹ï¼ˆç”¨äº JSON åºåˆ—åŒ–ï¼‰"""
    if isinstance(obj, dict):
        return {k: convert_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy(i) for i in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy(i) for i in obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return convert_numpy(obj.tolist())
    elif isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    else:
        return obj

# ==================== ç»˜å›¾å‡½æ•°ï¼ˆç”Ÿæˆ PDFï¼‰====================
def plot_svd_comparison(model_summary, save_path="svd_language_models.pdf"):
    """
    model_summary: dict {model_name: (eff_rank, max_ratio, ks)}
    """
    models = list(model_summary.keys())
    effs = [model_summary[m][0] for m in models]
    maxs = [model_summary[m][1] for m in models]
    kss = [model_summary[m][2] for m in models]

    # è®¡ç®—éšæœºåŸºçº¿ï¼ˆä½¿ç”¨åŒå°ºå¯¸é«˜æ–¯çŸ©é˜µï¼‰
    np.random.seed(42)
    baseline_effs, baseline_maxs, baseline_kss = [], [], []
    for _ in range(3):
        rand_mat = np.random.randn(256, 256)
        eff, mr, ks = compute_svd_metrics(rand_mat)
        if eff is not None:
            baseline_effs.append(eff)
            baseline_maxs.append(mr)
            baseline_kss.append(ks)
    baseline_eff = np.mean(baseline_effs) if baseline_effs else 0
    baseline_max = np.mean(baseline_maxs) if baseline_maxs else 0
    baseline_ks = np.mean(baseline_kss) if baseline_kss else 0

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    x = np.arange(len(models))

    # æœ‰æ•ˆç§©
    axes[0].bar(x, effs, color='steelblue', alpha=0.8)
    axes[0].axhline(y=baseline_eff, color='gray', linestyle='--', label=f'Random baseline ({baseline_eff:.1f})')
    axes[0].set_ylabel('Effective Rank')
    axes[0].set_title('Effective Rank (lower = more structured)')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(models, rotation=45, ha='right')
    axes[0].legend()

    # æœ€å¤§/å¹³å‡æ¯”
    axes[1].bar(x, maxs, color='coral', alpha=0.8)
    axes[1].axhline(y=baseline_max, color='gray', linestyle='--', label=f'Random baseline ({baseline_max:.2f})')
    axes[1].set_ylabel('Max/Mean Ratio')
    axes[1].set_title('Max/Mean Ratio (higher = more structured)')
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(models, rotation=45, ha='right')
    axes[1].legend()

    # KS è·ç¦»
    axes[2].bar(x, kss, color='mediumseagreen', alpha=0.8)
    axes[2].axhline(y=baseline_ks, color='gray', linestyle='--', label=f'Random baseline ({baseline_ks:.3f})')
    axes[2].set_ylabel('KS Distance')
    axes[2].set_title('KS Distance to Random')
    axes[2].set_xticks(x)
    axes[2].set_xticklabels(models, rotation=45, ha='right')
    axes[2].legend()

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å›¾è¡¨å·²ä¿å­˜: {save_path}")
    plt.show()

# ==================== æ‰¹é‡æ¨¡å‹åˆ†æ ====================
MODELS_TO_ANALYZE = [
    ("Qwen/Qwen2.5-0.5B", "model.safetensors", [0, 5, 11]),
    ("Qwen/Qwen2.5-1.5B", "model.safetensors", [0, 9, 17]),
    ("Qwen/Qwen2.5-3B", "model-00001-of-00002.safetensors", [0, 12, 23]),
    ("openai-community/gpt2", "model.safetensors", [0, 5, 11]),
    ("openai-community/gpt2-medium", "model.safetensors", [0, 11, 23]),
    ("openai-community/gpt2-large", "model.safetensors", [0, 17, 35]),
    ("TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model.safetensors", [0, 10, 21]),
    ("allenai/OLMo-1B-hf", "model.safetensors", [0, 7, 15]),
    ("EleutherAI/pythia-410m", "model.safetensors", [0, 11, 23]),
    ("EleutherAI/pythia-1b", "model.safetensors", [0, 7, 15]),
    ("bigscience/bloom-560m", "model.safetensors", [0, 11, 23]),
    # ä»¥ä¸‹ä¸¤ä¸ªæ¨¡å‹å¯èƒ½ä¸æ˜¯ safetensors æ ¼å¼ï¼Œå¯æš‚æ—¶æ³¨é‡Šæˆ–ä¿ç•™ï¼ˆå¦‚æœå‡ºé”™ä¼šæ•è·å¼‚å¸¸ï¼‰
    # ("facebook/opt-350m", "model.safetensors", [0, 11, 23]),
    # ("facebook/opt-1.3b", "model.safetensors", [0, 11, 23]),
]

print("ğŸš€ è¯­è¨€æ¨¡å‹ SVD åˆ†æï¼ˆä¸ä¸»å®éªŒä¸€è‡´çš„æ–¹æ³•ï¼‰")
print("=" * 70)
print(f"ç›®æ ‡æ¨¡å‹æ•°: {len(MODELS_TO_ANALYZE)}")
print(f"é¢„è®¡æ€»æ ·æœ¬æ•°: {sum(len(layers) for _, _, layers in MODELS_TO_ANALYZE)} (æ¯æ¨¡å‹3å±‚)")
print("=" * 70)

all_results = {}          # å­˜æ”¾å„å±‚è¯¦ç»†ç»“æœ
model_averages = {}       # å­˜æ”¾æ¯ä¸ªæ¨¡å‹çš„å¹³å‡æŒ‡æ ‡
total_analyzed = 0
total_errors = 0

for repo_id, filename, layer_indices in MODELS_TO_ANALYZE:
    model_name = repo_id.split('/')[-1]
    print(f"\n{'='*70}")
    print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
    print(f"{'='*70}")

    try:
        print(f"ğŸ“¥ åŠ è½½æƒé‡ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰...")
        file_path = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            resume_download=True,
            local_dir_use_symlinks=False,
            # å¦‚æœå¸Œæœ›å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œå¯å–æ¶ˆä¸‹ä¸€è¡Œçš„æ³¨é‡Š
            # local_files_only=True
        )
        print(f"   æœ¬åœ°è·¯å¾„: {file_path}")

        weights = load_file(file_path)

        # æŸ¥æ‰¾æ³¨æ„åŠ›å±‚æƒé‡ï¼ˆæ”¯æŒå¤šç§å‘½åè§„èŒƒï¼‰
        attn_patterns = [
            'self_attn.q_proj.weight',
            'attn.q_proj.weight',
            'attn.c_attn.weight',
            'attention.query_key_value.weight',
            'self_attention.query_key_value.weight'
        ]
        attn_keys = []
        for pattern in attn_patterns:
            found = [k for k in weights.keys() if pattern in k]
            if found:
                attn_keys = found
                break
        print(f"âœ… æ‰¾åˆ° {len(attn_keys)} ä¸ªæ³¨æ„åŠ›å±‚")

        if model_name not in all_results:
            all_results[model_name] = {}

        layer_effs, layer_maxs, layer_kss = [], [], []

        for layer_idx in layer_indices:
            if layer_idx < len(attn_keys):
                key = attn_keys[layer_idx]
                layer_name = f"Layer {layer_idx}"
                print(f"  ğŸ”¬ åˆ†æ {layer_name}...", end=' ')

                W = weights[key].float().numpy()
                eff, mr, ks = compute_svd_metrics(W)

                if eff is not None:
                    all_results[model_name][layer_name] = {
                        'eff_rank': eff,
                        'max_ratio': mr,
                        'ks_random': ks
                    }
                    layer_effs.append(eff)
                    layer_maxs.append(mr)
                    layer_kss.append(ks)
                    print(f"âœ… eff={eff:.2f}, mr={mr:.2f}, ks={ks:.4f}")
                    total_analyzed += 1
                else:
                    print(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥")
            else:
                print(f"  âš ï¸ Layer {layer_idx} ä¸å­˜åœ¨")

        if layer_effs:
            model_averages[model_name] = (
                np.mean(layer_effs),
                np.mean(layer_maxs),
                np.mean(layer_kss)
            )

    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
        total_errors += 1

# ==================== ç»Ÿè®¡æ±‡æ€» ====================
print(f"\n\n{'='*70}")
print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
print(f"{'='*70}")

for model_name, avg in model_averages.items():
    print(f"\nã€{model_name}ã€‘")
    print(f"  å¹³å‡æœ‰æ•ˆç§© = {avg[0]:.2f}")
    print(f"  å¹³å‡æœ€å¤§æ¯” = {avg[1]:.2f}")
    print(f"  å¹³å‡KSè·ç¦» = {avg[2]:.4f}")

print(f"\nâœ… æˆåŠŸåˆ†æ: {total_analyzed} ä¸ªæ ·æœ¬")
print(f"âŒ å¤±è´¥: {total_errors} ä¸ªæ¨¡å‹")

# ==================== ä¿å­˜ç»“æœåˆ° JSON ====================
results_json = {
    'timestamp': datetime.now().isoformat(),
    'total_samples': total_analyzed,
    'total_models': len(model_averages),
    'detailed_results': all_results,
    'model_averages': {
        model: {'eff_rank': avg[0], 'max_ratio': avg[1], 'ks_random': avg[2]}
        for model, avg in model_averages.items()
    }
}

results_json = convert_numpy(results_json)

with open('svd_language_models_analysis.json', 'w', encoding='utf-8') as f:
    json.dump(results_json, f, indent=2, ensure_ascii=False)
print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: svd_language_models_analysis.json")

# ==================== ç”Ÿæˆæ±‡æ€»å›¾ï¼ˆPDFï¼‰====================
if model_averages:
    plot_svd_comparison(model_averages, save_path="svd_language_models.pdf")
else:
    print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ¨¡å‹ç»“æœï¼Œæ— æ³•ç»˜å›¾")

print(f"\nâœ… åˆ†æå®Œæˆï¼")

import os

# ğŸ”¥ å…³é”®ï¼šå¿…é¡»åœ¨å¯¼å…¥ huggingface_hub ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼
HF_CACHE_DIR = r"D:\pythonstudy\huggingface_cache"
os.makedirs(HF_CACHE_DIR, exist_ok=True)
os.environ['HF_HOME'] = HF_CACHE_DIR
os.environ['HUGGINGFACE_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['HF_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_DIR
os.environ['HF_DATASETS_CACHE'] = HF_CACHE_DIR

import torch
import numpy as np
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file
import matplotlib.pyplot as plt
from scipy import stats
import json
from datetime import datetime

print(f"ğŸ’¾ Hugging Face ç¼“å­˜è·¯å¾„: {HF_CACHE_DIR}")
print(f"   HF_HOME: {os.environ['HF_HOME']}")
print(f"   HF_HUB_CACHE: {os.environ['HF_HUB_CACHE']}")
print(f"   (é¿å… C ç›˜ç©ºé—´ä¸è¶³)\n")

# ==================== ä¸ä¸»å®éªŒå®Œå…¨ä¸€è‡´çš„ GOE åˆ†æå‡½æ•° ====================
def analyze_layer_weights(W):
    """
    ä¸¥æ ¼æŒ‰ç…§ä¸»å®éªŒï¼ˆç´ æ•°æ¨¡å‹ï¼‰çš„æ–¹æ³•åˆ†æå•å±‚æƒé‡çŸ©é˜µ
    è¿”å› verdict, ks_goe, ks_poisson
    """
    # 1. ç¡®ä¿æ–¹é˜µï¼šå–å‰ min(rows,cols) è¡Œå’Œåˆ—
    rows, cols = W.shape
    n = min(rows, cols)
    W_square = W[:n, :n]

    # 2. å¯¹ç§°åŒ–
    H = (W_square + W_square.T) / 2

    # 3. ç‰¹å¾å€¼ï¼ˆå®æ•°ï¼‰
    eigvals = np.linalg.eigvalsh(H)
    eigvals = np.sort(eigvals)

    # 4. å–ä¸­é—´ 70% çš„ç‰¹å¾å€¼ï¼ˆå»æ‰ä¸¤ç«¯å„ 15%ï¼‰
    low = int(len(eigvals) * 0.15)
    high = int(len(eigvals) * 0.85)
    eigvals = eigvals[low:high]

    # 5. èƒ½çº§é—´è· & å½’ä¸€åŒ–
    spacings = np.diff(eigvals)
    if np.mean(spacings) == 0:
        return "Poisson", 1.0, 1.0  # å¼‚å¸¸æƒ…å†µ
    s = spacings / np.mean(spacings)

    # 6. æˆªæ–­åˆ° [0,4]ï¼ˆä¸ä¸»å®éªŒä¸€è‡´ï¼Œé¿å…æç«¯å€¼å½±å“ KSï¼‰
    s = s[s <= 4]

    # 7. GOE å’Œ Poisson çš„ CDF
    def goe_cdf(x):
        return 1 - np.exp(-np.pi * x ** 2 / 4)

    def poisson_cdf(x):
        return 1 - np.exp(-x)

    # 8. KS è·ç¦»
    ks_goe = stats.kstest(s, goe_cdf).statistic
    ks_poisson = stats.kstest(s, poisson_cdf).statistic

    # 9. åˆ¤å†³ï¼šç›´æ¥æ¯”è¾ƒ KS å€¼ï¼ˆä¸ä¸»å®éªŒåˆ¤å†³é€»è¾‘ä¸€è‡´ï¼‰
    verdict = "GOE" if ks_goe < ks_poisson else "Poisson"

    return verdict, ks_goe, ks_poisson


def plot_comparison(all_results, save_path="batch_analysis_summary.png"):
    """æ‰¹é‡ç»“æœå¯è§†åŒ–ï¼ˆä¸åŸæ¥ä¸€è‡´ï¼Œä»…éœ€ KS å€¼ï¼‰"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    models = []
    ks_goe_list = []
    ks_poisson_list = []
    verdicts = []

    for model_name, layers in all_results.items():
        for layer_name, res in layers.items():
            if layer_name == '_metadata' or not isinstance(res, dict) or 'ks_goe' not in res:
                continue
            models.append(f"{model_name}\n{layer_name}")
            ks_goe_list.append(res['ks_goe'])
            ks_poisson_list.append(res['ks_poisson'])
            verdicts.append(res['verdict'])

    # å›¾1ï¼šKSè·ç¦»å¯¹æ¯”ï¼ˆæ¡å½¢å›¾ï¼‰
    ax1 = axes[0, 0]
    x = np.arange(len(models))
    width = 0.35
    ax1.bar(x - width / 2, ks_goe_list, width, label='KS(GOE)', alpha=0.8, color='red')
    ax1.bar(x + width / 2, ks_poisson_list, width, label='KS(Poisson)', alpha=0.8, color='green')
    ax1.set_ylabel('KS Distance')
    ax1.set_title('KSç»Ÿè®¡é‡å¯¹æ¯”ï¼ˆè¶Šå°è¶Šå¥½ï¼‰')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models, rotation=45, ha='right', fontsize=8)
    ax1.legend()
    ax1.grid(alpha=0.3, axis='y')
    ax1.axhline(y=0.05, color='black', linestyle='--', linewidth=0.8, alpha=0.5)

    # å›¾2ï¼šåˆ¤å†³ç»Ÿè®¡ï¼ˆé¥¼å›¾ï¼‰
    ax2 = axes[0, 1]
    verdict_counts = {v: verdicts.count(v) for v in set(verdicts)}
    colors = {'GOE': 'red', 'Poisson': 'green'}
    ax2.pie(verdict_counts.values(), labels=verdict_counts.keys(), autopct='%1.1f%%',
            colors=[colors.get(k, 'gray') for k in verdict_counts.keys()],
            startangle=90)
    ax2.set_title(f'æ‰€æœ‰æ ·æœ¬åˆ¤å†³åˆ†å¸ƒ (n={len(models)})')

    # å›¾3ï¼šKSå·®å€¼
    ax3 = axes[1, 0]
    ks_diff = np.array(ks_goe_list) - np.array(ks_poisson_list)
    colors_bar = ['green' if d > 0 else 'red' for d in ks_diff]
    ax3.barh(models, ks_diff, color=colors_bar, alpha=0.7)
    ax3.set_xlabel('KS(GOE) - KS(Poisson)')
    ax3.set_title('Poissonä¼˜åŠ¿åº¦ï¼ˆæ­£å€¼=æ”¯æŒPoissonï¼‰')
    ax3.axvline(x=0, color='black', linewidth=1.5)
    ax3.grid(alpha=0.3, axis='x')
    ax3.tick_params(axis='y', labelsize=8)

    # å›¾4ï¼šæ•£ç‚¹å›¾
    ax4 = axes[1, 1]
    for v in set(verdicts):
        mask = [ver == v for ver in verdicts]
        color = 'red' if v == 'GOE' else 'green'
        ax4.scatter(np.array(ks_goe_list)[mask], np.array(ks_poisson_list)[mask],
                    label=v, alpha=0.7, s=100, color=color)
    ax4.plot([0, max(ks_goe_list + ks_poisson_list)],
             [0, max(ks_goe_list + ks_poisson_list)],
             'k--', linewidth=1, alpha=0.5)
    ax4.set_xlabel('KS(GOE)')
    ax4.set_ylabel('KS(Poisson)')
    ax4.set_title('KSè·ç¦»æ•£ç‚¹å›¾')
    ax4.legend()
    ax4.grid(alpha=0.3)
    ax4.set_aspect('equal')

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ æ±‡æ€»å›¾å·²ä¿å­˜: {save_path}")
    plt.show()


# ==================== æ‰¹é‡æ¨¡å‹åˆ†æ ====================
MODELS_TO_ANALYZE = [
    ("Qwen/Qwen2.5-0.5B", "model.safetensors", [0, 5, 11]),
    ("Qwen/Qwen2.5-1.5B", "model.safetensors", [0, 9, 17]),
    ("Qwen/Qwen2.5-3B", "model-00001-of-00002.safetensors", [0, 12, 23]),
    ("openai-community/gpt2", "model.safetensors", [0, 5, 11]),
    ("openai-community/gpt2-medium", "model.safetensors", [0, 11, 23]),
    ("openai-community/gpt2-large", "model.safetensors", [0, 17, 35]),
    ("TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model.safetensors", [0, 10, 21]),
    ("allenai/OLMo-1B-hf", "model.safetensors", [0, 7, 15]),
    ("EleutherAI/pythia-410m", "model.safetensors", [0, 11, 23]),
    ("EleutherAI/pythia-1b", "model.safetensors", [0, 7, 15]),
    ("bigscience/bloom-560m", "model.safetensors", [0, 11, 23]),
    ("facebook/opt-350m", "model.safetensors", [0, 11, 23]),
    ("facebook/opt-1.3b", "model.safetensors", [0, 11, 23]),
]

print("ğŸš€ å¤§è§„æ¨¡ Transformer æ¶æ„è°±åˆ†æï¼ˆä¸ä¸»å®éªŒä¸€è‡´çš„æ–¹æ³•ï¼‰")
print("=" * 70)
print(f"ç›®æ ‡æ¨¡å‹æ•°: {len(MODELS_TO_ANALYZE)}")
print(f"é¢„è®¡æ€»æ ·æœ¬æ•°: {sum(len(layers) for _, _, layers in MODELS_TO_ANALYZE)} (æ¯æ¨¡å‹3å±‚)")
print("=" * 70)

all_results = {}
total_analyzed = 0
total_errors = 0

for repo_id, filename, layer_indices in MODELS_TO_ANALYZE:
    model_name = repo_id.split('/')[-1]
    print(f"\n{'='*70}")
    print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
    print(f"{'='*70}")

    try:
        print(f"ğŸ“¥ ä¸‹è½½/åŠ è½½æƒé‡...")
        file_path = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            resume_download=True,
            local_dir_use_symlinks=False
        )
        print(f"   æœ¬åœ°è·¯å¾„: {file_path}")

        weights = load_file(file_path)

        # æŸ¥æ‰¾æ³¨æ„åŠ›å±‚æƒé‡ï¼ˆå¤šç§å‘½åè§„èŒƒï¼‰
        attn_patterns = [
            'self_attn.q_proj.weight',
            'attn.q_proj.weight',
            'attn.c_attn.weight',
            'attention.query_key_value.weight',
            'self_attention.query_key_value.weight'
        ]
        attn_keys = []
        for pattern in attn_patterns:
            found = [k for k in weights.keys() if pattern in k]
            if found:
                attn_keys = found
                break
        print(f"âœ… æ‰¾åˆ° {len(attn_keys)} ä¸ªæ³¨æ„åŠ›å±‚")

        if model_name not in all_results:
            all_results[model_name] = {
                '_metadata': {
                    'repo_id': repo_id,
                    'filename': filename,
                    'total_layers': len(attn_keys)
                }
            }

        # åˆ†ææŒ‡å®šå±‚
        for layer_idx in layer_indices:
            if layer_idx < len(attn_keys):
                key = attn_keys[layer_idx]
                layer_name = f"Layer {layer_idx}"
                print(f"  ğŸ”¬ åˆ†æ {layer_name}...", end=' ')

                W = weights[key].float().numpy()
                verdict, ks_goe, ks_poisson = analyze_layer_weights(W)

                all_results[model_name][layer_name] = {
                    'verdict': verdict,
                    'ks_goe': ks_goe,
                    'ks_poisson': ks_poisson
                }

                print(f"âœ… {verdict} (KS_GOE={ks_goe:.4f}, KS_Poisson={ks_poisson:.4f})")
                total_analyzed += 1
            else:
                print(f"  âš ï¸ Layer {layer_idx} ä¸å­˜åœ¨")

    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
        total_errors += 1

# ==================== ç»Ÿè®¡æ±‡æ€» ====================
print(f"\n\n{'='*70}")
print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
print(f"{'='*70}")

all_verdicts = []
all_ks_goe = []
all_ks_poisson = []

for model_name, layers in all_results.items():
    print(f"\nã€{model_name}ã€‘")
    for layer_name, res in layers.items():
        if layer_name == '_metadata':
            continue
        if not isinstance(res, dict) or 'verdict' not in res:
            continue
        print(f"  {layer_name}: {res['verdict']} (KS_GOE={res['ks_goe']:.4f}, KS_Poisson={res['ks_poisson']:.4f})")
        all_verdicts.append(res['verdict'])
        all_ks_goe.append(res['ks_goe'])
        all_ks_poisson.append(res['ks_poisson'])

print(f"\nâœ… æˆåŠŸåˆ†æ: {total_analyzed} ä¸ªæ ·æœ¬")
print(f"âŒ å¤±è´¥: {total_errors} ä¸ªæ¨¡å‹")

# æ€»ä½“åˆ¤å†³åˆ†å¸ƒ
verdict_counts = {}
for v in all_verdicts:
    verdict_counts[v] = verdict_counts.get(v, 0) + 1
print("\nğŸ“ˆ æ€»ä½“åˆ¤å†³åˆ†å¸ƒ:")
for verdict, count in verdict_counts.items():
    print(f"   {verdict}: {count} ({count/len(all_verdicts)*100:.1f}%)")

print("\nğŸ“‰ KSè·ç¦»ç»Ÿè®¡:")
print(f"   GOE:     å‡å€¼={np.mean(all_ks_goe):.4f}, æ ‡å‡†å·®={np.std(all_ks_goe):.4f}")
print(f"   Poisson: å‡å€¼={np.mean(all_ks_poisson):.4f}, æ ‡å‡†å·®={np.std(all_ks_poisson):.4f}")

# é…å¯¹tæ£€éªŒ
from scipy.stats import ttest_rel
t_stat, p_value = ttest_rel(all_ks_goe, all_ks_poisson)
print(f"\nğŸ”¬ é…å¯¹tæ£€éªŒ (GOE vs Poisson): t={t_stat:.4f}, p={p_value:.4e}")
if p_value < 0.001:
    winner = "Poisson" if t_stat > 0 else "GOE"
    print(f"   âœ… {winner} æ˜¾è‘—æ›´ä¼˜ (p<0.001)")

# ä¿å­˜ç»“æœ
results_json = {
    'timestamp': datetime.now().isoformat(),
    'total_samples': total_analyzed,
    'verdict_distribution': verdict_counts,
    'ks_statistics': {
        'goe_mean': float(np.mean(all_ks_goe)),
        'poisson_mean': float(np.mean(all_ks_poisson))
    },
    'ttest': {'t_statistic': float(t_stat), 'p_value': float(p_value)},
    'detailed_results': {
        model: {layer: res for layer, res in layers.items() if layer != '_metadata'}
        for model, layers in all_results.items()
    }
}
with open('transformer_spectra_analysis.json', 'w', encoding='utf-8') as f:
    json.dump(results_json, f, indent=2, ensure_ascii=False)
print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: transformer_spectra_analysis.json")

# ç”Ÿæˆæ±‡æ€»å›¾
plot_comparison(all_results, save_path="transformer_batch_analysis.png")

print("\nğŸ¯ æœ€ç»ˆç»“è®º")
if verdict_counts.get('Poisson', 0) > total_analyzed * 0.7:
    print("âœ… Transformer æ¶æ„æ•´ä½“å±•ç° Poisson åˆ†å¸ƒç‰¹å¾")
    print("   è¿™æ„å‘³ç€ä½ çš„ç´ æ•°æ¨¡å‹çš„ GOE ç‰¹å¾å¾ˆå¯èƒ½æ˜¯ç´ æ•°ç‰¹æœ‰çš„ï¼")
elif verdict_counts.get('GOE', 0) > total_analyzed * 0.7:
    print("âš ï¸ Transformer æ¶æ„æ•´ä½“å±•ç° GOE åˆ†å¸ƒç‰¹å¾")
    print("   è¿™ä¼šå‰Šå¼±ç´ æ•°æ¨¡å‹ GOE ç‰¹å¾çš„ç‹¬ç‰¹æ€§ï¼Œéœ€è¿›ä¸€æ­¥æ£€æŸ¥")
else:
    print("âš ï¸ ç»“æœæ··åˆï¼Œéœ€è¿›ä¸€æ­¥åˆ†æ")

print(f"\nâœ… åˆ†æå®Œæˆï¼å…±å¤„ç† {total_analyzed} ä¸ªæ ·æœ¬")

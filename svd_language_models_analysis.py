import os

# ğŸ”¥ å…³é”®ï¼šå¿…é¡»åœ¨å¯¼å…¥ huggingface_hub ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼
HF_CACHE_DIR = r"D:\pythonstudy\huggingface_cache"
os.makedirs(HF_CACHE_DIR, exist_ok=True)
os.environ['HF_HOME'] = HF_CACHE_DIR
os.environ['HUGGINGFACE_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['HF_HUB_CACHE'] = os.path.join(HF_CACHE_DIR, 'hub')
os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_DIR
os.environ['HF_DATASETS_CACHE'] = HF_CACHE_DIR

import torch
import numpy as np
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file
import matplotlib.pyplot as plt
from scipy import stats
import json
from datetime import datetime

print(f"ğŸ’¾ Hugging Face ç¼“å­˜è·¯å¾„: {HF_CACHE_DIR}")
print(f"   HF_HOME: {os.environ['HF_HOME']}")
print(f"   HF_HUB_CACHE: {os.environ['HF_HUB_CACHE']}")
print(f"   (é¿å… C ç›˜ç©ºé—´ä¸è¶³)\n")


# ==================== SVD åˆ†æå‡½æ•°ï¼ˆä¸ä¸»å®éªŒä¸€è‡´ï¼‰====================
def analyze_svd(W):
    """
    è¾“å…¥æ–¹é˜µ Wï¼Œè®¡ç®— SVD ç›¸å…³æŒ‡æ ‡ï¼š
    - æœ‰æ•ˆç§© (effective rank)
    - æœ€å¤§å¥‡å¼‚å€¼ä¸å¹³å‡å¥‡å¼‚å€¼ä¹‹æ¯”
    - ä¸åŒå°ºå¯¸éšæœºé«˜æ–¯çŸ©é˜µçš„ KS è·ç¦»
    """
    U, S, Vh = np.linalg.svd(W, full_matrices=False)

    # æœ‰æ•ˆç§©
    total = np.sum(S)
    p = S / total
    entropy = -np.sum(p * np.log(p + 1e-12))
    eff_rank = np.exp(entropy)

    # æœ€å¤§/å¹³å‡æ¯”
    max_s = S[0]
    mean_s = np.mean(S)
    max_ratio = max_s / mean_s

    # ä¸éšæœºåŸºçº¿çš„ KS è·ç¦»ï¼ˆå›ºå®šç§å­ä¿è¯å¯å¤ç°ï¼‰
    np.random.seed(42)
    W_rand = np.random.randn(W.shape[0], W.shape[1])
    U_rand, S_rand, Vh_rand = np.linalg.svd(W_rand, full_matrices=False)
    ks_random = stats.ks_2samp(S, S_rand).statistic

    return eff_rank, max_ratio, ks_random, S


def plot_comparison(all_results, save_path="batch_analysis_summary.png"):
    """æ‰¹é‡ç»“æœå¯è§†åŒ–ï¼ˆåŸºäº SVD æŒ‡æ ‡ï¼‰"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    models = []
    eff_ranks = []
    max_ratios = []
    ks_randoms = []

    for model_name, res in all_results.items():
        models.append(model_name)
        eff_ranks.append(res['eff_rank'])
        max_ratios.append(res['max_ratio'])
        ks_randoms.append(res['ks_random'])

    x = np.arange(len(models))

    # å›¾1ï¼šæœ‰æ•ˆç§©
    ax1 = axes[0, 0]
    ax1.bar(x, eff_ranks, color='blue', alpha=0.7)
    ax1.set_ylabel('Effective Rank')
    ax1.set_title('Effective Rank (lower = more structured)')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models, rotation=45, ha='right', fontsize=8)
    ax1.grid(alpha=0.3, axis='y')
    ax1.axhline(y=256, color='black', linestyle='--', linewidth=0.8, alpha=0.5, label='Full rank')
    ax1.legend()

    # å›¾2ï¼šæœ€å¤§/å¹³å‡å¥‡å¼‚å€¼æ¯”
    ax2 = axes[0, 1]
    ax2.bar(x, max_ratios, color='red', alpha=0.7)
    ax2.set_ylabel('Max/Mean Ratio')
    ax2.set_title('Max/Mean Singular Value Ratio (higher = more structured)')
    ax2.set_xticks(x)
    ax2.set_xticklabels(models, rotation=45, ha='right', fontsize=8)
    ax2.grid(alpha=0.3, axis='y')
    ax2.axhline(y=3, color='black', linestyle='--', linewidth=0.8, alpha=0.5, label='Random baseline (~2-3)')
    ax2.legend()

    # å›¾3ï¼šä¸éšæœºåŸºçº¿çš„ KS è·ç¦»
    ax3 = axes[1, 0]
    ax3.bar(x, ks_randoms, color='green', alpha=0.7)
    ax3.set_ylabel('KS Distance to Random')
    ax3.set_title('KS Distance from Random SVD (higher = more structured)')
    ax3.set_xticks(x)
    ax3.set_xticklabels(models, rotation=45, ha='right', fontsize=8)
    ax3.grid(alpha=0.3, axis='y')
    ax3.axhline(y=0.05, color='black', linestyle='--', linewidth=0.8, alpha=0.5, label='Random similarity')
    ax3.legend()

    # å›¾4ï¼šæ•£ç‚¹å›¾ï¼ˆæœ‰æ•ˆç§© vs æœ€å¤§æ¯”ï¼‰
    ax4 = axes[1, 1]
    ax4.scatter(eff_ranks, max_ratios, alpha=0.7, s=100)
    for i, model in enumerate(models):
        ax4.annotate(model, (eff_ranks[i], max_ratios[i]), fontsize=6, alpha=0.7)
    ax4.set_xlabel('Effective Rank')
    ax4.set_ylabel('Max/Mean Ratio')
    ax4.set_title('Effective Rank vs Max/Mean Ratio')
    ax4.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ æ±‡æ€»å›¾å·²ä¿å­˜: {save_path}")
    plt.show()


# è¾…åŠ©å‡½æ•°ï¼šå°† numpy ç±»å‹è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹ä»¥ä¾¿ JSON åºåˆ—åŒ–
def convert_numpy(obj):
    if isinstance(obj, np.float32) or isinstance(obj, np.float64):
        return float(obj)
    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
        return int(obj)
    elif isinstance(obj, dict):
        return {k: convert_numpy(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy(i) for i in obj]
    else:
        return obj


# ==================== æ‰¹é‡æ¨¡å‹åˆ†æ ====================
MODELS_TO_ANALYZE = [
    ("Qwen/Qwen2.5-0.5B", "model.safetensors"),
    ("Qwen/Qwen2.5-1.5B", "model.safetensors"),
    ("Qwen/Qwen2.5-3B", "model-00001-of-00002.safetensors"),
    ("openai-community/gpt2", "model.safetensors"),
    ("openai-community/gpt2-medium", "model.safetensors"),
    ("openai-community/gpt2-large", "model.safetensors"),
    ("TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model.safetensors"),
    ("allenai/OLMo-1B-hf", "model.safetensors"),
    ("EleutherAI/pythia-410m", "model.safetensors"),
    ("EleutherAI/pythia-1b", "model.safetensors"),
    ("bigscience/bloom-560m", "model.safetensors"),
    # OPT ç³»åˆ—æ–‡ä»¶å¯èƒ½ä¸æ˜¯ safetensorsï¼Œæš‚æ—¶è·³è¿‡
    # ("facebook/opt-350m", "model.safetensors"),
    # ("facebook/opt-1.3b", "model.safetensors"),
]

print("ğŸš€ å¤§è§„æ¨¡ Transformer æ¶æ„ SVD åˆ†æï¼ˆä¸ä¸»å®éªŒä¸€è‡´çš„æ–¹æ³•ï¼‰")
print("=" * 70)
print(f"ç›®æ ‡æ¨¡å‹æ•°: {len(MODELS_TO_ANALYZE)}")
print("=" * 70)

all_results = {}
total_analyzed = 0
total_errors = 0

for repo_id, filename in MODELS_TO_ANALYZE:
    model_name = repo_id.split('/')[-1]
    print(f"\n{'=' * 70}")
    print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
    print(f"{'=' * 70}")

    try:
        print(f"ğŸ“¥ ä¸‹è½½/åŠ è½½æƒé‡...")
        file_path = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            resume_download=True,
            local_dir_use_symlinks=False
        )
        print(f"   æœ¬åœ°è·¯å¾„: {file_path}")

        weights = load_file(file_path)

        # æŸ¥æ‰¾æ‰€æœ‰æ³¨æ„åŠ›å±‚æƒé‡çŸ©é˜µï¼ˆå¤šç§å‘½åè§„èŒƒï¼‰
        attn_patterns = [
            'self_attn.q_proj.weight',
            'attn.q_proj.weight',
            'attn.c_attn.weight',
            'attention.query_key_value.weight',
            'self_attention.query_key_value.weight'
        ]
        weight_matrices = []
        for pattern in attn_patterns:
            found = [weights[k] for k in weights.keys() if pattern in k]
            if found:
                weight_matrices.extend(found)
                break  # å‡è®¾åªåŒ¹é…ä¸€ç§æ¨¡å¼ï¼Œæ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒå‘½å
        print(f"âœ… æ”¶é›†åˆ° {len(weight_matrices)} ä¸ªæ³¨æ„åŠ›å±‚æƒé‡çŸ©é˜µ")

        if len(weight_matrices) == 0:
            print("  âš ï¸ æœªæ‰¾åˆ°æ³¨æ„åŠ›å±‚æƒé‡ï¼Œè·³è¿‡")
            continue

        # å°†æ‰€æœ‰çŸ©é˜µè½¬ä¸º numpy å¹¶æ£€æŸ¥ç»´åº¦
        np_matrices = []
        for w in weight_matrices:
            w_np = w.float().cpu().numpy()
            # ç¡®ä¿æ˜¯äºŒç»´ä¸”è‡³å°‘æœ‰ä¸€ç»´ >=256ï¼ˆä¾¿äºæˆªå–ï¼‰
            if w_np.ndim == 2 and w_np.shape[0] >= 256 and w_np.shape[1] >= 256:
                np_matrices.append(w_np)
            else:
                print(f"  âš ï¸ è·³è¿‡å½¢çŠ¶ {w_np.shape} çš„çŸ©é˜µï¼ˆè‡³å°‘éœ€è¦256ç»´ï¼‰")

        if len(np_matrices) == 0:
            print("  âš ï¸ æ²¡æœ‰æ»¡è¶³ç»´åº¦è¦æ±‚çš„çŸ©é˜µï¼Œè·³è¿‡")
            continue

        # æ‹¼æ¥æ‰€æœ‰çŸ©é˜µï¼ˆæ²¿ç¬¬ä¸€ç»´ï¼‰
        W_huge = np.concatenate(np_matrices, axis=0)

        # ç¡®ä¿è‡³å°‘æœ‰256è¡Œå’Œ256åˆ—
        if W_huge.shape[0] < 256 or W_huge.shape[1] < 256:
            print(f"  âš ï¸ æ‹¼æ¥åçŸ©é˜µå½¢çŠ¶ {W_huge.shape} å°äº256ï¼Œæ— æ³•åˆ†æ")
            continue

        # æˆªå–å‰256è¡Œå’Œå‰256åˆ—
        W = W_huge[:256, :256]

        # æ‰§è¡Œ SVD åˆ†æ
        eff_rank, max_ratio, ks_random, S = analyze_svd(W)

        all_results[model_name] = {
            'eff_rank': eff_rank,
            'max_ratio': max_ratio,
            'ks_random': ks_random
        }
        print(f"  âœ… æœ‰æ•ˆç§© = {eff_rank:.2f}, æœ€å¤§/å¹³å‡æ¯” = {max_ratio:.2f}, KSéšæœºè·ç¦» = {ks_random:.4f}")
        total_analyzed += 1

    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
        total_errors += 1

# ==================== ç»Ÿè®¡æ±‡æ€» ====================
print(f"\n\n{'=' * 70}")
print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
print(f"{'=' * 70}")

eff_ranks = []
max_ratios = []
ks_randoms = []

for model_name, res in all_results.items():
    print(f"\nã€{model_name}ã€‘")
    print(f"  æœ‰æ•ˆç§© = {res['eff_rank']:.2f}")
    print(f"  æœ€å¤§/å¹³å‡å¥‡å¼‚å€¼æ¯” = {res['max_ratio']:.2f}")
    print(f"  KSè·ç¦» vs éšæœºåŸºçº¿ = {res['ks_random']:.4f}")
    eff_ranks.append(res['eff_rank'])
    max_ratios.append(res['max_ratio'])
    ks_randoms.append(res['ks_random'])

print(f"\nâœ… æˆåŠŸåˆ†æ: {total_analyzed} ä¸ªæ¨¡å‹")
print(f"âŒ å¤±è´¥: {total_errors} ä¸ªæ¨¡å‹")

print("\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
print(f"  æœ‰æ•ˆç§©: å‡å€¼={np.mean(eff_ranks):.2f}, æ ‡å‡†å·®={np.std(eff_ranks):.2f}")
print(f"  æœ€å¤§/å¹³å‡æ¯”: å‡å€¼={np.mean(max_ratios):.2f}, æ ‡å‡†å·®={np.std(max_ratios):.2f}")
print(f"  KSéšæœºè·ç¦»: å‡å€¼={np.mean(ks_randoms):.4f}, æ ‡å‡†å·®={np.std(ks_randoms):.4f}")

# ä¿å­˜ç»“æœåˆ° JSONï¼ˆå…ˆè½¬æ¢ numpy ç±»å‹ï¼‰
results_json = {
    'timestamp': datetime.now().isoformat(),
    'total_models': total_analyzed,
    'statistics': {
        'eff_rank_mean': float(np.mean(eff_ranks)),
        'eff_rank_std': float(np.std(eff_ranks)),
        'max_ratio_mean': float(np.mean(max_ratios)),
        'max_ratio_std': float(np.std(max_ratios)),
        'ks_random_mean': float(np.mean(ks_randoms)),
        'ks_random_std': float(np.std(ks_randoms)),
    },
    'detailed_results': all_results
}

# è½¬æ¢ numpy ç±»å‹
results_json = convert_numpy(results_json)

with open('transformer_svd_analysis.json', 'w', encoding='utf-8') as f:
    json.dump(results_json, f, indent=2, ensure_ascii=False)
print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: transformer_svd_analysis.json")

# ç”Ÿæˆæ±‡æ€»å›¾
plot_comparison(all_results, save_path="transformer_batch_analysis.png")

print("\nğŸ¯ æœ€ç»ˆç»“è®º")
print("åˆ†æå®Œæˆã€‚å¯å°†è¿™äº› SVD æŒ‡æ ‡ä¸ç´ æ•°æ¨¡å‹ï¼ˆPlan A/B/Cï¼‰çš„ç»“æœè¿›è¡Œå¯¹æ¯”ã€‚")
print(f"\nâœ… åˆ†æå®Œæˆï¼å…±å¤„ç† {total_analyzed} ä¸ªæ ·æœ¬")
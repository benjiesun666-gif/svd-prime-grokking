"""
æ‰¹é‡ç”Ÿæˆ Plan A/B/C çš„å¯¹æ¯”å›¾
- é¢„æµ‹ç²¾åº¦åˆ†æï¼ˆå¾®è§‚è§†å›¾ + ç›¸å…³æ€§ï¼‰
- SVDè°±åˆ†æï¼ˆæœ‰æ•ˆç§©ã€æœ€å¤§å¥‡å¼‚å€¼æ¯”ã€ä¸éšæœºåŸºçº¿çš„KSè·ç¦»ï¼‰
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sympy import primerange
import os
from pathlib import Path

# ==================== é…ç½®åŒº ====================
WEIGHT_DIR = r"D:\pythonstudy\python_task\æƒé‡åˆ†æ"
OUTPUT_DIR = r"D:\pythonstudy\python_task\æƒé‡åˆ†æ\è®ºæ–‡å›¾ç‰‡"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å®éªŒé…ç½®
EXPERIMENTS = {
    "Plan_A": {
        "weight_file": "Plan A.pt",
        "learnable_embedding": True,
        "label": "Plan A (Learnable + Large Batch)",
        "color": "#E74C3C"  # çº¢è‰²
    },
    "Plan_B": {
        "weight_file": "Plan B.pt",
        "learnable_embedding": True,
        "label": "Plan B (Learnable + Small Batch)",
        "color": "#3498DB"  # è“è‰²
    },
    "Plan_C": {
        "weight_file": "Plan C.pt",
        "learnable_embedding": False,  # æ­£å¼¦æ³¢ç¼–ç 
        "label": "Plan C (Sinusoidal + Large Batch)",
        "color": "#2ECC71"  # ç»¿è‰²
    }
}

# æ¨¡å‹å‚æ•°
D_MODEL = 256
N_LAYERS = 6
N_HEADS = 8
DROPOUT = 0.1
NUM_PRIMES = 1000000

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}\n")

# ==================== æ¨¡å‹å®šä¹‰ ====================
class RiemannEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000000, learnable=True):
        super().__init__()
        self.learnable = learnable
        if learnable:
            self.embedding = nn.Embedding(max_len, d_model)
        else:
            # æ­£å¼¦æ³¢ç¼–ç 
            pe = torch.zeros(max_len, d_model)
            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            self.register_buffer('pe', pe)

    def forward(self, x):
        if self.learnable:
            return self.embedding(x)
        else:
            return self.pe[x]

class PrimeGapPredictor(nn.Module):
    def __init__(self, learnable_embedding=True):
        super().__init__()
        self.riemann_embedding = RiemannEmbedding(D_MODEL, NUM_PRIMES, learnable=learnable_embedding)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=D_MODEL,
            nhead=N_HEADS,
            dim_feedforward=D_MODEL*4,
            dropout=DROPOUT,
            batch_first=True,
            norm_first=True,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=N_LAYERS)
        self.output = nn.Sequential(
            nn.Linear(D_MODEL, D_MODEL // 2),
            nn.GELU(),
            nn.Dropout(DROPOUT),
            nn.Linear(D_MODEL // 2, D_MODEL // 4),
            nn.GELU(),
            nn.Dropout(DROPOUT),
            nn.Linear(D_MODEL // 4, 1)
        )

    def forward(self, x):
        embedded = self.riemann_embedding(x).unsqueeze(1)
        transformed = self.transformer(embedded)
        return self.output(transformed.squeeze(1))

# ==================== åˆ†æå‡½æ•° ====================

def load_model(weight_path, learnable_embedding):
    """åŠ è½½æ¨¡å‹"""
    model = PrimeGapPredictor(learnable_embedding=learnable_embedding).to(device)
    checkpoint = torch.load(weight_path, map_location=device, weights_only=False)
    state = checkpoint['model_state_dict'] if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint
    model.load_state_dict(state)
    model.eval()
    return model

def analyze_prediction_accuracy(model, plan_name, color):
    """é¢„æµ‹ç²¾åº¦åˆ†æï¼ˆå®Œå…¨ç…§æŠ„æºä»£ç ï¼‰"""
    print(f"  ğŸ” åˆ†æé¢„æµ‹ç²¾åº¦...")

    # ç”Ÿæˆå…¨éƒ¨ç´ æ•°å¹¶è®¡ç®—å…¨å±€ç»Ÿè®¡é‡
    all_primes = list(primerange(1, 18500000))[:NUM_PRIMES]
    all_gaps = np.diff(all_primes)

    global_mean = np.mean(all_gaps)
    global_std = np.std(all_gaps)

    # å–æœ€å1000ä¸ªgap
    total_gaps = len(all_gaps)
    target_len = 1000
    start_idx = total_gaps - target_len
    end_idx = total_gaps
    target_gaps = all_gaps[start_idx:end_idx]

    # é¢„æµ‹
    indices = torch.arange(start_idx, end_idx, device=device)
    with torch.no_grad():
        preds_norm = model(indices).squeeze().cpu().numpy()

    # è¿˜åŸå½’ä¸€åŒ–
    preds_real = (preds_norm * global_std) + global_mean

    # è®¡ç®—è¯¯å·®
    mae = np.mean(np.abs(preds_real - target_gaps))

    # ç»˜å›¾
    plt.figure(figsize=(18, 6))

    # å·¦å›¾ï¼šå¾®è§‚è§†å›¾ (å‰200ä¸ª)
    plt.subplot(1, 2, 1)
    plt.plot(target_gaps[:200], color='black', alpha=0.6, label='Real Truth', linewidth=2)
    plt.plot(preds_real[:200], color='red', alpha=0.8, linestyle='--', label='AI Prediction', linewidth=1.5)
    plt.title(f'Micro View: First 200 of Last 1000 Gaps')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å³å›¾ï¼šæ•´ä½“ç›¸å…³æ€§
    plt.subplot(1, 2, 2)
    plt.scatter(target_gaps, preds_real, alpha=0.5, s=10, c='blue')
    min_v = min(target_gaps.min(), preds_real.min())
    max_v = max(target_gaps.max(), preds_real.max())
    plt.plot([min_v, max_v], [min_v, max_v], 'r--', label='Perfect Fit')
    plt.title(f'Correlation (MAE={mae:.4f})')
    plt.xlabel('Real Gap')
    plt.ylabel('Predicted Gap')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, f'{plan_name}_prediction.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"    âœ… MAE = {mae:.4f}, å›¾ç‰‡å·²ä¿å­˜")
    return mae

def analyze_svd_properties(model, plan_name, color):
    """SVDè°±åˆ†æï¼šè®¡ç®—æœ‰æ•ˆç§©ã€æœ€å¤§å¥‡å¼‚å€¼æ¯”ã€ä¸éšæœºåŸºçº¿çš„KSè·ç¦»"""
    print(f"  ğŸ”¬ åˆ†æSVDè°±...")

    # æå–æƒé‡
    weights = []
    for name, param in model.named_parameters():
        if 'in_proj_weight' in name:
            weights.append(param.detach().cpu().numpy())

    if not weights:
        for name, param in model.named_parameters():
            if len(param.shape)==2 and param.shape[0]==param.shape[1]:
                weights.append(param.detach().cpu().numpy())

    if not weights:
        print("    âš ï¸ æœªæ‰¾åˆ°å¯ç”¨æƒé‡çŸ©é˜µ")
        return None

    # æ‹¼æ¥å¹¶æˆªå–ï¼ˆåŒä¸»å®éªŒæ–¹æ³•ï¼‰
    W_huge = np.concatenate(weights, axis=0)
    n = min(2048, W_huge.shape[0], W_huge.shape[1])
    W = W_huge[:n, :n]  # 256x256ï¼Œä¸è¿›è¡Œå¯¹ç§°åŒ–

    # è®¡ç®—å¥‡å¼‚å€¼
    U, S, Vh = np.linalg.svd(W, full_matrices=False)
    # S å·²æ’åºä¸ºé™åº

    # 1. æœ‰æ•ˆç§© (effective rank)
    total = np.sum(S)
    p = S / total
    entropy = -np.sum(p * np.log(p + 1e-12))
    eff_rank = np.exp(entropy)

    # 2. æœ€å¤§å¥‡å¼‚å€¼ä¸å¹³å‡å¥‡å¼‚å€¼ä¹‹æ¯”
    max_s = S[0]
    mean_s = np.mean(S)
    max_ratio = max_s / mean_s

    # 3. ä¸éšæœºåŸºçº¿çš„KSè·ç¦»
    # ç”Ÿæˆä¸€ä¸ªç›¸åŒå°ºå¯¸çš„éšæœºé«˜æ–¯çŸ©é˜µï¼Œè®¡ç®—å…¶å¥‡å¼‚å€¼
    np.random.seed(42)  # å›ºå®šç§å­ï¼Œä¿è¯å¯å¤ç°
    W_random = np.random.randn(n, n)
    U_rand, S_rand, Vh_rand = np.linalg.svd(W_random, full_matrices=False)
    # è®¡ç®—ä¸¤ç»„å¥‡å¼‚å€¼åˆ†å¸ƒçš„KSè·ç¦»
    ks_random = stats.ks_2samp(S, S_rand).statistic

    # ç»˜å›¾ï¼šå¥‡å¼‚å€¼è°±
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.semilogy(S, 'o-', markersize=3, linewidth=1, color=color, label=f'{plan_name} singular values')
    ax.semilogy(S_rand, '--', linewidth=2, color='gray', alpha=0.7, label='Random baseline')
    ax.set_title(f'{plan_name}: Singular Value Spectrum', fontsize=14, fontweight='bold')
    ax.set_xlabel('Index', fontsize=12)
    ax.set_ylabel('Singular Value (log scale)', fontsize=12)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, f'{plan_name}_svd_spectrum.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"    âœ… æœ‰æ•ˆç§© = {eff_rank:.2f}")
    print(f"      æœ€å¤§/å¹³å‡å¥‡å¼‚å€¼æ¯” = {max_ratio:.2f}")
    print(f"      KSè·ç¦» vs éšæœºåŸºçº¿ = {ks_random:.4f}")

    return {
        "eff_rank": eff_rank,
        "max_ratio": max_ratio,
        "ks_random": ks_random
    }

# ==================== ä¸»æµç¨‹ ====================

def main():
    print("="*60)
    print("ğŸš€ æ‰¹é‡ç”Ÿæˆ Plan A/B/C å¯¹æ¯”å›¾")
    print("="*60)

    results = {}

    for plan_name, config in EXPERIMENTS.items():
        print(f"\nğŸ“¦ å¤„ç† {plan_name}...")

        weight_path = os.path.join(WEIGHT_DIR, config["weight_file"])
        if not os.path.exists(weight_path):
            print(f"  âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")
            continue

        print(f"  â³ åŠ è½½æƒé‡: {config['weight_file']}")
        model = load_model(weight_path, config["learnable_embedding"])

        mae = analyze_prediction_accuracy(model, plan_name, config["color"])
        svd_result = analyze_svd_properties(model, plan_name, config["color"])

        results[plan_name] = {
            "mae_pred": mae,
            "svd": svd_result
        }

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š å®éªŒæ±‡æ€»")
    print("="*60)
    for plan_name, result in results.items():
        print(f"\nã€{plan_name}ã€‘")
        print(f"  é¢„æµ‹ç²¾åº¦: MAE = {result['mae_pred']:.4f}")
        if result['svd']:
            print(f"  SVDè°±åˆ†æ:")
            print(f"    - æœ‰æ•ˆç§© = {result['svd']['eff_rank']:.2f}")
            print(f"    - æœ€å¤§/å¹³å‡å¥‡å¼‚å€¼æ¯” = {result['svd']['max_ratio']:.2f}")
            print(f"    - KSè·ç¦» vs éšæœºåŸºçº¿ = {result['svd']['ks_random']:.4f}")

    print("\n" + "="*60)
    print(f"âœ… æ‰€æœ‰å›¾ç‰‡å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
    print("="*60)

if __name__ == "__main__":
    main()